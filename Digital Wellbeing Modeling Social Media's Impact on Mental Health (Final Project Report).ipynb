{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Project Report\n",
    "#### STATS 102 Introduction to Data Science Session 2, 2025 Fall    beijing time\n",
    "\n",
    "You can zip everything including the data (or box link) and the codes into a zip file and submit through canvas/assignment/final_project\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Digital Wellbeing: Modeling Social Media's Impact on Mental Health\n",
    "\n",
    "- YIFEI PAN (yp167@duke.edu)\n",
    "- RUANYIYANG SUN (rs788@duke.edu)\n",
    "- TIANQI YIN (ty144@duke.edu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Introduction\n",
    "\n",
    "In an era where digital connectivity is omnipresent, social media platforms have transformed from simple communication tools into complex ecosystems that significantly influence our daily lives, psychological well-being, and self-perception. For university students—who represent one of the most digitally engaged demographics—navigating this landscape presents both unprecedented opportunities for self-expression and potential risks to mental health. This project investigates the nuanced relationship between social media usage patterns, lifestyle habits, and psychological outcomes among young adults, leveraging data science methodologies to quantify what has often been discussed only qualitatively.\n",
    "\n",
    "Our research addresses a critical gap in contemporary digital wellness literature: while extensive studies have examined social media’s correlation with mental health indicators, few have employed multivariate modeling techniques to predict well-being from behavioral patterns or to identify actionable thresholds for healthier digital engagement. We bridge this gap by analyzing a comprehensive dataset tracking screen time, platform preferences, stress levels, sleep quality, exercise frequency, and self-reported happiness among students. Through this analysis, we move beyond simple correlation to build predictive models that can distinguish between high-risk and low-risk mental health profiles based on digital behavior patterns.\n",
    "\n",
    "The primary objectives of this study are threefold. First, we conduct exploratory data analysis to identify key patterns and relationships between social media usage and well-being metrics. Second, we develop and compare multiple machine learning models—including Naive Bayes, Gradient Boosting, Linear Discriminant Analysis (LDA), and Support Vector Machines (SVM)—to predict happiness levels and mental health risk categories. Third, we employ feature selection techniques like LASSO regression to distill the most influential predictors from a broader set of lifestyle variables, creating interpretable models that offer practical insights rather than merely black-box predictions.\n",
    "\n",
    "Our methodological approach is deliberately multi-model: we begin with simple baseline models to establish performance benchmarks, then progress to more sophisticated algorithms that capture non-linear relationships and feature interactions. This progression allows us to balance model interpretability with predictive accuracy, ensuring our findings are both statistically robust and practically meaningful. Through Principal Component Analysis (PCA), we create composite well-being and lifestyle scores that capture the multidimensional nature of these constructs, while LDA enables clear visualization of how different happiness levels separate in reduced-dimensional space.\n",
    "\n",
    "This work contributes to the growing field of digital wellness analytics by demonstrating that everyday social media habits—when analyzed through appropriate data science lenses—reveal systematic patterns with measurable psychological consequences. Our findings not only validate the connection between screen time management and mental health but also provide evidence-based thresholds that could inform university wellness programs, digital literacy curricula, and individual behavior modification strategies. Ultimately, this project represents an interdisciplinary synthesis of psychological theory, behavioral data, and statistical modeling, offering a template for how data science can illuminate one of the most pressing wellness challenges of the digital age."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Background\n",
    "\n",
    "College students spend many hours on social media every day, while also dealing with academic pressure, irregular sleep, and changing social networks. Prior communication research shows that online platforms are not only sources of entertainment but also spaces for self-presentation and identity work. Studies on social capital and self-presentation argue that social networking sites help young people maintain both strong and weak ties and manage how they appear to others, which in turn shapes their sense of belonging and well-being (Chu & Choi, 2010). Other work suggests that the Internet can make it easier to express aspects of the “true self” that are hard to show offline (Bargh, McKenna, & Fitzsimons, 2002) and that online profiles often involve strategic impression management rather than a simple copy of offline identity (Bullingham & Vasconcelos, 2013). Together, these studies imply that the mental-health impact of social media depends not only on “how much time” students spend online, but also on how they use these platforms and how this use fits into broader lifestyles.\n",
    "\n",
    "Most quantitative studies, however, use a relatively simple modeling strategy. They often reduce mental health to a single binary outcome (e.g., “issue” vs. “no issue”) and then apply standard classifiers such as logistic regression, random forests, or gradient boosting using raw features like daily screen time or sleep duration. These models can achieve high predictive accuracy, but they usually treat well-being as a black-box label and give little insight into which patterns of behavior are most strongly associated with better or worse outcomes. As a result, they answer the question “Can we predict who is at risk?” but do not fully address “What combination of behaviors seems healthier?” or “Where might realistic intervention thresholds lie for real students?”\n",
    "\n",
    "Our project builds on this literature by using the Mental Health and Social Media Balance dataset (N = 500) to construct a more interpretable and behaviorally meaningful view of student well-being. First, we use principal component analysis (PCA) to create composite scores for well-being (combining stress, sleep quality, and happiness) and for lifestyle (combining screen time, exercise, and days without social media). This allows us to move beyond a single survey item and instead model latent patterns of “healthier vs. less healthy” lifestyles and mental states. \n",
    "\n",
    "In Project 1, we then apply Lasso regression and logistic regression to these PCA-based scores in order to estimate which behavioral factors matter most and to identify a data-driven decision boundary that separates a “Good status” group from a more vulnerable group. Project 2 focuses on mental health risk as a binary outcome and compares two modeling philosophies: a simple probabilistic model (Naive Bayes) and a more complex tree-based ensemble (Gradient Boosting). By evaluating their performance on the same set of lifestyle and social-media features, we ask not only which model predicts risk more accurately, but also how much extra complexity is actually necessary for this type of student-level data. Project 3 zooms in on subjective happiness as a multi-class label and uses Lasso for feature selection followed by Linear Discriminant Analysis (LDA) to separate low, medium, and high happiness groups. This step emphasizes interpretability: LDA provides linear combinations of features that best separate happiness levels, which can be translated into concrete behavior patterns (for example, combinations of screen time and exercise frequency) instead of opaque model scores.\n",
    "\n",
    "Taken together, these three sub-projects extend previous social-science research by providing a quantitative, interpretable map of how everyday digital behaviors cluster into healthier or less healthy lifestyles for college students. Rather than treating social media as uniformly harmful or beneficial, our models highlight specific regions of behavior—such as screen-time thresholds and exercise levels—where the probability of being in a “Good status” or high-happiness group changes most sharply. This creates a bridge between qualitative theories of self-presentation and social capital and practical, data-driven guidance about how students might adjust their daily habits to support their mental health."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Design and Implementation\n",
    "\n",
    "### Data:\n",
    "Our dataset is from https://www.kaggle.com/datasets/prince7489/mental-health-and-social-media-balance-dataset\n",
    "\n",
    "## Selecting models\n",
    "\n",
    "### 1. Model Selection Philosophy\n",
    "We adopt a **multi‑model, task‑driven** approach.  \n",
    "The choice of model is guided by:\n",
    "- **Problem type** (regression, binary classification, multi‑class classification)\n",
    "- **Interpretability needs**\n",
    "- **Non‑linearity in the data**\n",
    "- **Feature‑selection requirements**\n",
    "\n",
    "### 2. The Models We Used and Why\n",
    "\n",
    "| Task | Primary Model(s) | Reason for Selection |\n",
    "|------|------------------|----------------------|\n",
    "| Binary mental‑health risk | Naive Bayes (baseline)<br>Gradient Boosting (improved) | Baseline simplicity vs. higher accuracy |\n",
    "| Multi‑class happiness level | LDA (Linear Discriminant Analysis)<br>SVM on LDA‑projected space | Maximizes class separation; adds non‑linear flexibility |\n",
    "| Feature selection & continuous prediction | LASSO Regression | L1 regularization shrinks irrelevant coefficients to zero |\n",
    "| Binary classification with sparse features | Logistic Regression (L1 & L2) | Combines interpretability with built‑in feature selection |\n",
    "\n",
    "### 3. Supporting Formulas\n",
    "\n",
    "**LASSO (for feature selection)**  \n",
    "minimize : 1/(2n) × ||y – Xw||²₂ + α × ||w||₁\n",
    "- **Why it matters**: The L1 penalty (`||w||₁`) forces weak coefficients to zero, leaving only the most predictive features.\n",
    "\n",
    "**LDA (for class separation)**  \n",
    "J(w) = wᵀS_B w / wᵀS_W w\n",
    "- **Why it matters**: Maximizes between‑class variance (`S_B`) while minimizing within‑class variance (`S_W`), yielding projections that best separate groups.\n",
    "\n",
    "**Model‑Evaluation Metrics**  \n",
    "- Accuracy = (TP + TN) / (TP + TN + FP + FN)  \n",
    "- F1 Score = 2 × (Precision × Recall) / (Precision + Recall)  \n",
    "- ROC‑AUC = ∫ TPR(FPR) dFPR  \n",
    "\n",
    "These formulas underpin all quantitative comparisons between models.\n",
    "\n",
    "### 4. Key Figures That Guided Our Choices\n",
    "\n",
    "**Figure 1 – LASSO Regularization Path**  \n",
    "- *Shows* how coefficients shrink as regularization strength (α) increases.\n",
    "- *Used to* identify the α that retains only the strongest predictors.\n",
    "\n",
    "**Figure 2 – LDA Projection (2D Scatter Plot)**  \n",
    "- *Shows* clear separation of “low”, “medium”, and “high” happiness groups.\n",
    "- *Justifies* using LDA for multi‑class problems; the first discriminant function explains >99% of variance.\n",
    "\n",
    "**Figure 3 – Model‑Comparison Bar Chart**  \n",
    "- *Compares* Naive Bayes vs. Gradient Boosting on Accuracy, Precision, Recall, F1.\n",
    "- *Evidence* for choosing Gradient Boosting: it consistently outperforms the baseline.\n",
    "\n",
    "**Figure 4 – ROC Curves**  \n",
    "- *Plots* True Positive Rate vs. False Positive Rate for both models.\n",
    "- *Shows* Gradient Boosting achieves higher AUC, confirming its better discrimination ability.\n",
    "\n",
    "**Figure 5 – Confusion Matrices Side‑by‑Side**  \n",
    "- *Visualizes* where each model makes errors (False Positives / False Negatives).\n",
    "- *Helps* select the model that minimizes the most costly errors for the application.\n",
    "\n",
    "### 5. Our Step‑by‑Step Selection Process\n",
    "\n",
    "**Step 1 – Feature Screening**  \n",
    "- Run LASSO to identify `Stress_Level`, `Sleep_Quality`, `Daily_Screen_Time` as top features.\n",
    "- Use these features in all subsequent models.\n",
    "\n",
    "**Step 2 – Baseline vs. Advanced Comparison**  \n",
    "- Train Naive Bayes (simple, fast) and Gradient Boosting (complex, powerful).\n",
    "- Compare metrics and ROC curves; choose Gradient Boosting for final deployment.\n",
    "\n",
    "**Step 3 – Multi‑Class Strategy**  \n",
    "- Apply LDA to project data into a space that maximizes class separation.\n",
    "- Feed LDA projections into SVM to capture any remaining non‑linear boundaries.\n",
    "\n",
    "**Step 4 – Regularization Tuning**  \n",
    "- Use cross‑validation to pick the best `C` (inverse regularization strength) for logistic regression.\n",
    "- Balance model complexity against overfitting.\n",
    "\n",
    "### 6. Why These Choices Are Supported\n",
    "\n",
    "1. **LASSO coefficients** → Quantitative proof that only 3–4 features drive predictions.\n",
    "2. **LDA explained‑variance ratio** ([0.992, 0.008]) → Proof that 1D projection is sufficient.\n",
    "3. **Side‑by‑side performance metrics** → Clear, numerical advantage of Gradient Boosting.\n",
    "4. **ROC‑AUC values** → Gradient Boosting’s higher AUC shows better overall ranking of risk.\n",
    "\n",
    "### 7. Final Model‑Selection Summary\n",
    "\n",
    "| Model | Best For | Key Supporting Evidence |\n",
    "|-------|----------|--------------------------|\n",
    "| Gradient Boosting | Binary mental‑health risk prediction | Higher accuracy (≈74 %) & F1; better ROC curve |\n",
    "| LDA + SVM | Multi‑class happiness level | 99 % variance explained by first LD; clear visual separation |\n",
    "| LASSO | Feature selection & continuous wellbeing score | Coefficient‑shrinkage path; only 3 non‑zero coefficients |\n",
    "| Logistic Regression | Interpretable binary classification | Cross‑validated `C`; clean decision boundary (screen‑time ≈ 4.5 hrs) |\n",
    "\n",
    "The figures and formulas above provide a **reproducible, visual, and quantitative** rationale for every modeling decision in the project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Results\n",
    "\n",
    "Write your results and discussions here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Results and Discussion\n",
    "\n",
    "### 5.1 PCA: Constructing Well-being and Lifestyle Scores\n",
    "\n",
    "We first used principal component analysis (PCA) to build composite scores for well-being and lifestyle.\n",
    "\n",
    "For **Wellbeing_Score**, we combined sleep quality, stress level (with the sign flipped so that higher values mean lower stress), and the happiness index. The first principal component explained **77.9%** of the total variance, and after orienting the component so that higher values mean better well-being, it correlated strongly with the original happiness index (r = **0.92**). This suggests that the PCA score is a good latent summary of the three self-report items.\n",
    "\n",
    "For **Lifestyle_Score**, we combined daily screen time (sign-flipped so that less time is “better”), days without social media (also flipped), and exercise frequency. The first component explained **37.0%** of the variance. Lifestyle_Score was **positively** correlated with Wellbeing_Score (r ≈ **0.59**), indicating that healthier digital and physical habits tend to co-occur with higher well-being. At the same time, the correlation is far from perfect, leaving room for individual differences and other unmeasured factors.\n",
    "\n",
    "---\n",
    "\n",
    "### 5.2 Project 1: Lasso and Logistic Regression for *Good_status*\n",
    "\n",
    "In **Project 1**, we used the three lifestyle variables as predictors and the PCA-based Wellbeing_Score as the outcome in a Lasso regression model (α = 0.1). After standardization, the model achieved a test **MSE of 0.62** and **R² of 0.73**, meaning that about 73% of the variance in well-being can be explained by these three everyday behaviors.\n",
    "\n",
    "The coefficients were:\n",
    "\n",
    "- Daily_Screen_Time(hrs): **–1.17**  \n",
    "- Days_Without_Social_Media: **≈ 0**  \n",
    "- Exercise_Frequency(week): **≈ 0**\n",
    "\n",
    "This pattern shows that **daily screen time dominates** the linear relationship with well-being, while the additional predictive power of days without social media and exercise is comparatively small once screen time is controlled for.\n",
    "\n",
    "To translate the continuous score into a more interpretable outcome, we defined a binary label **Good_status**, equal to 1 if a student’s Wellbeing_Score is at or above the sample median and 0 otherwise. We first fitted an L1-penalized logistic regression, which retained **Daily_Screen_Time(hrs)** and **Exercise_Frequency(week)** and dropped Days_Without_Social_Media. We then re-fitted an L2-penalized logistic model on these selected features.\n",
    "\n",
    "On the held-out test set, the final logistic model achieved:\n",
    "\n",
    "- Accuracy: **0.77**  \n",
    "- Precision for Good_status = 1: **0.74**  \n",
    "- Recall for Good_status = 1: **0.84**  \n",
    "- F1-score for Good_status = 1: **0.79**  \n",
    "\n",
    "The confusion matrix (rows = true labels, columns = predicted labels) was:\n",
    "\n",
    "\\[\n",
    "\\begin{bmatrix}\n",
    "35 & 15 \\\\\n",
    "8 & 42\n",
    "\\end{bmatrix}\n",
    "\\]\n",
    "\n",
    "Using the standardized coefficients and the mean and standard deviation of screen time, we derived a **decision boundary** for daily screen time (holding exercise at its mean). At a predicted probability of 0.5, the cut-off is approximately **5.6 hours per day**. Students below this threshold are more likely to be in the good-status group, while students above this threshold are more likely to fall below the median well-being level. This suggests that the key issue is not simply “using social media” but **using it for more than about five to six hours per day**.\n",
    "\n",
    "---\n",
    "\n",
    "### 5.3 Project 2: Predicting Binary Mental-Health Risk\n",
    "\n",
    "In **Project 2**, we reframed the problem as a binary classification task. We defined **mental_health_risk = 1** if a student’s happiness index was **6 or below** and 0 otherwise. In our sample, **12.4%** of students were labeled high risk.\n",
    "\n",
    "As predictors, we used age, gender, daily screen time, sleep quality, stress level, days without social media, exercise frequency, and primary social-media platform. Categorical variables were one-hot encoded, and numeric variables were standardized when needed.\n",
    "\n",
    "A trivial baseline that **always predicts “low risk”** reached an accuracy of **0.876**, but it had **zero recall** for high-risk students, making it useless for screening.\n",
    "\n",
    "We then compared two models:\n",
    "\n",
    "- **Gaussian Naive Bayes** (with standardized inputs)  \n",
    "  - Accuracy: **0.873**  \n",
    "  - Precision (high-risk class): **0.50**  \n",
    "  - Recall (high-risk class): **0.68**  \n",
    "  - F1-score (high-risk class): **0.58**  \n",
    "  - ROC–AUC: **0.87**  \n",
    "  - Confusion matrix: \\(\\begin{bmatrix}118 & 13 \\\\ 6 & 13\\end{bmatrix}\\)\n",
    "\n",
    "- **Gradient Boosting Classifier** (100 trees, max depth = 3)  \n",
    "  - Accuracy: **0.880**  \n",
    "  - Precision (high-risk class): **0.53**  \n",
    "  - Recall (high-risk class): **0.47**  \n",
    "  - F1-score (high-risk class): **0.50**  \n",
    "  - ROC–AUC: **0.83**  \n",
    "  - Confusion matrix: \\(\\begin{bmatrix}123 & 8 \\\\ 10 & 9\\end{bmatrix}\\)\n",
    "\n",
    "Both models clearly outperform the majority baseline in terms of identifying high-risk students. Naive Bayes tends to **catch more high-risk cases** (higher recall) at the cost of more false positives, while Gradient Boosting is **more conservative**, with fewer false positives but also more missed high-risk students. For a mental-health screening context—where missing at-risk students is especially costly—the Naive Bayes model may be preferable despite its slightly lower overall accuracy.\n",
    "\n",
    "---\n",
    "\n",
    "### 5.4 Project 3: Lasso Feature Selection and LDA for Happiness Groups\n",
    "\n",
    "In **Project 3**, we considered subjective happiness as a **multi-class** outcome. We grouped the happiness index into three categories:\n",
    "\n",
    "- **Low happiness**: scores **≤ 6**  \n",
    "- **Medium happiness**: scores **7–8**  \n",
    "- **High happiness**: scores **9–10**\n",
    "\n",
    "We applied **LassoCV** to the same set of demographic and behavioral variables (with one-hot encoding and standardization) to predict the numeric happiness score. The optimal regularization strength (α ≈ 0.032) produced a sparse solution. The three coefficients with the largest absolute values were:\n",
    "\n",
    "- Stress_Level(1–10): **–0.69**  \n",
    "- Sleep_Quality(1–10): **+0.46**  \n",
    "- Daily_Screen_Time(hrs): **–0.18**\n",
    "\n",
    "Again, **stress, sleep quality, and daily screen time** emerged as the most influential factors.\n",
    "\n",
    "Using only these three features (standardized), we fitted a **Linear Discriminant Analysis (LDA)** model to classify students into low, medium, and high happiness groups. On a 10% test split, LDA achieved an overall **accuracy of 0.70**. Class-wise performance was:\n",
    "\n",
    "- High happiness: precision **0.79**, recall **0.73**  \n",
    "- Medium happiness: precision **0.57**, recall **0.72**  \n",
    "- Low happiness: precision **1.00**, recall **0.50**\n",
    "\n",
    "The first discriminant function accounted for **99.6%** of the between-group variance, indicating that a single underlying dimension—roughly, **“good sleep and short screen time vs. high stress and long screen time”**—captures most of the separation among the three happiness levels.\n",
    "\n",
    "---\n",
    "\n",
    "### 5.5 Integrated Discussion\n",
    "\n",
    "Across the three projects, several consistent themes emerge:\n",
    "\n",
    "1. **Lifestyle and well-being are strongly linked.**  \n",
    "   The positive correlation between Lifestyle_Score and Wellbeing_Score (r ≈ 0.59) shows that healthier digital and physical habits are systematically associated with better well-being. However, the correlation is moderate rather than perfect, suggesting that individual differences and contextual factors still matter.\n",
    "\n",
    "2. **Daily screen time is the central behavioral predictor.**  \n",
    "   In both Lasso regression and logistic regression, daily screen time has a much larger impact on well-being than days without social media or exercise frequency. The logistic model provides a concrete threshold of about **5.6 hours per day**, beyond which the probability of being in the good-status group drops sharply.\n",
    "\n",
    "3. **Simple models can be effective and interpretable.**  \n",
    "   Naive Bayes and LDA—both relatively simple and transparent—achieve competitive performance compared to a more complex Gradient Boosting model. Naive Bayes, in particular, offers high recall for high-risk students and a high ROC–AUC, making it suitable as an initial screening tool.\n",
    "\n",
    "4. **There is clear heterogeneity in how students are affected.**  \n",
    "   Even with strong aggregate patterns, some high–screen-time students still report high happiness, and some low–screen-time students report low happiness. This heterogeneity is consistent with social-science work on online identity and self-presentation, which emphasizes that social media can be supportive for some users and harmful for others, depending on how it is integrated into their broader lifestyle and social networks.\n",
    "\n",
    "Overall, the results suggest that social media use is neither uniformly “good” nor uniformly “bad.” Instead, its impact depends on **usage intensity** and on how it interacts with stress, sleep, and other aspects of students’ daily routines.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ruanyiyang Sun:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Descriptive analysis reveals clear patterns: higher screen time, lower sleep quality, and elevated stress levels often cluster among users with low happiness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================================================\n",
    "#        Data Exploration And Descriptive Analysis\n",
    "# ===============================================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# -------------------------- 1. Data Loading and Basic Checks --------------------------\n",
    "print(\"=\"*60)\n",
    "print(\"MENTAL HEALTH AND SOCIAL MEDIA USAGE DATA ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "df = pd.read_csv(\"Mental_Health_and_Social_Media_Balance_Dataset.csv\")\n",
    "print(f\"Dataset size: {df.shape[0]} rows, {df.shape[1]} columns\")\n",
    "print(f\"Columns: {list(df.columns)}\")\n",
    "print(\"\\n1. BASIC DATA INFORMATION:\")    # Check basic information and missing values\n",
    "print(df.info())\n",
    "print(f\"\\nMISSING VALUES:\")\n",
    "missing_values = df.isnull().sum()\n",
    "print(missing_values[missing_values > 0])\n",
    "if \"Happiness_Index(1-10)\" in df.columns:    # Create target variable: mental health risk (Happiness Index ≤ 6 indicates risk)\n",
    "    df[\"mental_health_risk\"] = (df[\"Happiness_Index(1-10)\"] <= 6).astype(int)\n",
    "    print(f\"\\nCreated target variable: mental_health_risk\")\n",
    "    print(f\"  High risk (Happiness ≤ 6): {df['mental_health_risk'].sum()} people ({(df['mental_health_risk'].sum()/len(df)*100):.1f}%)\")\n",
    "    print(f\"  Low risk: {(len(df)-df['mental_health_risk'].sum())} people ({((len(df)-df['mental_health_risk'].sum())/len(df)*100):.1f}%)\")\n",
    "\n",
    "# -------------------------- 2. Descriptive Statistical Analysis --------------------------\n",
    "print(\"\\n2. DESCRIPTIVE STATISTICS:\")\n",
    "print(\"-\"*40)\n",
    "numeric_features = []    # Select numerical features\n",
    "for col in df.columns:\n",
    "    if df[col].dtype in ['int64', 'float64']:\n",
    "        if col not in ['User_ID', 'mental_health_risk']:\n",
    "            numeric_features.append(col)\n",
    "print(f\"Numerical features: {numeric_features}\")\n",
    "if numeric_features:      # Calculate basic statistics\n",
    "    desc_stats = df[numeric_features].describe().T\n",
    "    desc_stats['skewness'] = df[numeric_features].apply(lambda x: stats.skew(x.dropna()))\n",
    "    desc_stats['kurtosis'] = df[numeric_features].apply(lambda x: stats.kurtosis(x.dropna()))\n",
    "    print(\"\\nNumerical Feature Statistics:\")\n",
    "    print(desc_stats.round(3))\n",
    "else:\n",
    "    print(\"No numerical features found\")\n",
    "print(\"\\nCATEGORICAL FEATURE DISTRIBUTION:\")    # Categorical feature distribution\n",
    "categorical_features = []\n",
    "for col in df.columns:\n",
    "    if df[col].dtype == 'object':\n",
    "        categorical_features.append(col)\n",
    "if categorical_features:\n",
    "    for feature in categorical_features:\n",
    "        print(f\"\\n{feature} Distribution:\")\n",
    "        print(df[feature].value_counts())\n",
    "else:\n",
    "    print(\"No categorical features found\")\n",
    "\n",
    "# -------------------------- 3. Data Visualization Analysis --------------------------\n",
    "print(\"\\n3. DATA VISUALIZATION ANALYSIS:\")\n",
    "print(\"-\"*40)\n",
    "fig = plt.figure(figsize=(18, 12))    # Create multiple subplots\n",
    "fig.suptitle('Mental Health Data Analysis - Descriptive Statistics and Distributions', fontsize=16, fontweight='bold')\n",
    "for i, feature in enumerate(numeric_features[:9]):  # Show maximum 9 features   # Use compatible method for distribution plots\n",
    "    ax = plt.subplot(3, 4, i+1)\n",
    "    ax.hist(df[feature].dropna(), bins=20, color='skyblue', alpha=0.7, edgecolor='black')\n",
    "    from scipy.stats import gaussian_kde    # Add KDE curve\n",
    "    data = df[feature].dropna()\n",
    "    if len(data) > 1:\n",
    "        kde = gaussian_kde(data)\n",
    "        x_range = np.linspace(data.min(), data.max(), 100)\n",
    "        ax.plot(x_range, kde(x_range) * len(data) * (x_range[1] - x_range[0]), color='darkblue', linewidth=2)\n",
    "    mean_val = df[feature].mean()    # Add statistical information\n",
    "    median_val = df[feature].median()\n",
    "    ax.axvline(mean_val, color='red', linestyle='--', alpha=0.7, label=f'Mean: {mean_val:.2f}')\n",
    "    ax.axvline(median_val, color='green', linestyle='--', alpha=0.7, label=f'Median: {median_val:.2f}')\n",
    "    ax.set_title(f'{feature} Distribution', fontsize=12)\n",
    "    ax.set_xlabel('')\n",
    "    ax.set_ylabel('Frequency')\n",
    "    if i == 0:\n",
    "        ax.legend(fontsize=8)\n",
    "ax_risk = plt.subplot(3, 4, 7)   # 3.2 Risk group comparison plot\n",
    "if \"mental_health_risk\" in df.columns and len(numeric_features) > 0:     # Calculate mean values for each feature in both groups\n",
    "    risk_means = df[df['mental_health_risk']==1][numeric_features].mean()\n",
    "    no_risk_means = df[df['mental_health_risk']==0][numeric_features].mean()\n",
    "    display_features = numeric_features[:6]     # Select first few features to display\n",
    "    x = np.arange(len(display_features))\n",
    "    width = 0.35\n",
    "    risk_vals = [risk_means[f] for f in display_features]\n",
    "    no_risk_vals = [no_risk_means[f] for f in display_features]\n",
    "    ax_risk.bar(x - width/2, no_risk_vals, width, label='Low Risk', alpha=0.8, color='lightblue')\n",
    "    ax_risk.bar(x + width/2, risk_vals, width, label='High Risk', alpha=0.8, color='lightcoral')\n",
    "    ax_risk.set_xlabel('Feature')\n",
    "    ax_risk.set_ylabel('Mean Value')\n",
    "    ax_risk.set_title('High Risk vs Low Risk Group Comparison')\n",
    "    ax_risk.set_xticks(x)\n",
    "    ax_risk.set_xticklabels([f[:10]+'...' if len(f)>10 else f for f in display_features], rotation=45)\n",
    "    ax_risk.legend()\n",
    "ax_corr = plt.subplot(3, 4, 8)    # 3.3 Correlation heatmap\n",
    "if numeric_features and \"mental_health_risk\" in df.columns:    # Select features for correlation calculation\n",
    "    corr_features = numeric_features[:6] + ['mental_health_risk']  # Limit number of features\n",
    "    corr_matrix = df[corr_features].corr()\n",
    "    sns.heatmap(corr_matrix, annot=True, fmt='.2f', cmap='coolwarm', center=0, \n",
    "                square=True, linewidths=1, cbar_kws={'shrink': 0.8}, ax=ax_corr)\n",
    "    ax_corr.set_title('Feature Correlation Heatmap')\n",
    "else:\n",
    "    ax_corr.text(0.5, 0.5, 'Cannot calculate correlation\\n(Missing numerical features or target variable)', \n",
    "                ha='center', va='center', transform=ax_corr.transAxes)\n",
    "    ax_corr.set_title('Feature Correlation Heatmap')\n",
    "ax_gender = plt.subplot(3, 4, 9)      # 3.4 Gender distribution plot\n",
    "if 'Gender' in df.columns:\n",
    "    gender_counts = df['Gender'].value_counts()\n",
    "    colors = ['lightblue', 'lightcoral', 'lightgreen']\n",
    "    ax_gender.pie(gender_counts.values, labels=gender_counts.index, autopct='%1.1f%%',\n",
    "                 colors=colors[:len(gender_counts)], startangle=90)\n",
    "    ax_gender.set_title('Gender Distribution')\n",
    "else:\n",
    "    ax_gender.text(0.5, 0.5, 'No gender data available', \n",
    "                  ha='center', va='center', transform=ax_gender.transAxes)\n",
    "    ax_gender.set_title('Gender Distribution')\n",
    "ax_platform = plt.subplot(3, 4, 10)   # 3.5 Social media platform distribution\n",
    "if 'Social_Media_Platform' in df.columns:\n",
    "    platform_counts = df['Social_Media_Platform'].value_counts()\n",
    "    ax_platform.bar(range(len(platform_counts)), platform_counts.values, color='lightseagreen')\n",
    "    ax_platform.set_xlabel('Platform')\n",
    "    ax_platform.set_ylabel('Number of Users')\n",
    "    ax_platform.set_title('Social Media Platform Usage Distribution')\n",
    "    ax_platform.set_xticks(range(len(platform_counts)))\n",
    "    ax_platform.set_xticklabels(platform_counts.index, rotation=45)\n",
    "else:\n",
    "    ax_platform.text(0.5, 0.5, 'No platform data available', \n",
    "                    ha='center', va='center', transform=ax_platform.transAxes)\n",
    "    ax_platform.set_title('Social Media Platform Usage Distribution')\n",
    "ax_age_screen = plt.subplot(3, 4, 11)     # 3.6 Age vs Screen Time relationship\n",
    "if 'Age' in df.columns and 'Daily_Screen_Time(hrs)' in df.columns:\n",
    "    if 'mental_health_risk' in df.columns:\n",
    "        sns.scatterplot(data=df, x='Age', y='Daily_Screen_Time(hrs)', \n",
    "                       hue='mental_health_risk', alpha=0.6, ax=ax_age_screen)\n",
    "    else:\n",
    "        sns.scatterplot(data=df, x='Age', y='Daily_Screen_Time(hrs)', \n",
    "                       alpha=0.6, ax=ax_age_screen)\n",
    "    ax_age_screen.set_xlabel('Age')\n",
    "    ax_age_screen.set_ylabel('Daily Screen Time (hours)')\n",
    "    ax_age_screen.set_title('Age vs Screen Time Relationship')\n",
    "else:\n",
    "    ax_age_screen.text(0.5, 0.5, 'Missing required data', \n",
    "                      ha='center', va='center', transform=ax_age_screen.transAxes)\n",
    "    ax_age_screen.set_title('Age vs Screen Time Relationship')\n",
    "ax_stress_sleep = plt.subplot(3, 4, 12)    # 3.7 Stress vs Sleep Quality relationship\n",
    "if 'Stress_Level(1-10)' in df.columns and 'Sleep_Quality(1-10)' in df.columns:\n",
    "    if 'mental_health_risk' in df.columns:\n",
    "        sns.scatterplot(data=df, x='Stress_Level(1-10)', y='Sleep_Quality(1-10)', \n",
    "                       hue='mental_health_risk', alpha=0.6, ax=ax_stress_sleep)\n",
    "    else:\n",
    "        sns.scatterplot(data=df, x='Stress_Level(1-10)', y='Sleep_Quality(1-10)', \n",
    "                       alpha=0.6, ax=ax_stress_sleep)\n",
    "    ax_stress_sleep.set_xlabel('Stress Level (1-10)')\n",
    "    ax_stress_sleep.set_ylabel('Sleep Quality (1-10)')\n",
    "    ax_stress_sleep.set_title('Stress Level vs Sleep Quality Relationship')\n",
    "else:\n",
    "    ax_stress_sleep.text(0.5, 0.5, 'Missing required data', \n",
    "                        ha='center', va='center', transform=ax_stress_sleep.transAxes)\n",
    "    ax_stress_sleep.set_title('Stress Level vs Sleep Quality Relationship')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# -------------------------- 4. Key Findings Summary --------------------------\n",
    "print(\"\\n4. KEY FINDINGS SUMMARY:\")\n",
    "print(\"-\"*40)\n",
    "if 'mental_health_risk' in df.columns and len(numeric_features) > 0:\n",
    "    risk_group = df[df['mental_health_risk'] == 1]    # High-risk group characteristics\n",
    "    no_risk_group = df[df['mental_health_risk'] == 0]\n",
    "    print(f\"High-risk group ({len(risk_group)} people, {len(risk_group)/len(df)*100:.1f}%):\")\n",
    "    key_features = ['Daily_Screen_Time(hrs)', 'Sleep_Quality(1-10)', \n",
    "                    'Stress_Level(1-10)', 'Exercise_Frequency(week)']    # Display statistics for key features\n",
    "    for feature in key_features:\n",
    "        if feature in df.columns:\n",
    "            risk_mean = risk_group[feature].mean()\n",
    "            no_risk_mean = no_risk_group[feature].mean()\n",
    "            diff = risk_mean - no_risk_mean\n",
    "            print(f\"  • {feature}: {risk_mean:.1f} (vs Low risk: {no_risk_mean:.1f}, Difference: {diff:+.1f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yifei Pan:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA-based analysis of the relationship between lifestyle\n",
    "Idea:\n",
    "1. Build a one-dimensional \"Wellbeing_Score\" from three outcome variables using PCA.\n",
    "2. Build a one-dimensional \"Lifestyle_Score\" from three behavioral variables using PCA.\n",
    "3. Compute the correlation between these two scores and visualize their relationship in a scatter plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "df=pd.read_csv('Mental_Health_and_Social_Media_Balance_Dataset.csv')\n",
    "\n",
    "# 1. Construct a PCA-based well-being score\n",
    "# (Sleep quality ↑, Stress ↓, Happiness ↑)\n",
    "\n",
    "# Select outcome variables related to well-being\n",
    "Y_cols = [\"Sleep_Quality(1-10)\", \"Stress_Level(1-10)\", \"Happiness_Index(1-10)\"]\n",
    "Y = df.loc[:, Y_cols].copy()\n",
    "\n",
    "# Flip the sign of stress so that higher values always mean \"better\"\n",
    "Y[\"Stress_Level(1-10)\"] = -Y[\"Stress_Level(1-10)\"]\n",
    "\n",
    "# Standardize well-being variables (mean = 0, std = 1)\n",
    "scaler_Y = StandardScaler()\n",
    "Y_z = scaler_Y.fit_transform(Y)\n",
    "\n",
    "# Apply PCA and keep only the first principal component\n",
    "pca_Y = PCA(n_components=1)\n",
    "Y_pc1 = pca_Y.fit_transform(Y_z)\n",
    "\n",
    "# Store the first principal component as a composite well-being score\n",
    "df[\"Wellbeing_Score\"] = Y_pc1[:, 0]\n",
    "\n",
    "print(\"Well-being PCA explained variance ratio:\", pca_Y.explained_variance_ratio_)\n",
    "print(\"Well-being PCA weights:\", pd.Series(pca_Y.components_[0], index=Y_cols))\n",
    "\n",
    "\n",
    "# 2. Construct a PCA-based lifestyle score\n",
    "#    (Screen time, social-media breaks, exercise)\n",
    "# Select behavioral/lifestyle variables\n",
    "X_cols = [\"Daily_Screen_Time(hrs)\", \"Days_Without_Social_Media\", \"Exercise_Frequency(week)\"]\n",
    "X = df.loc[:, X_cols].copy()\n",
    "\n",
    "# Optionally flip some variables so that larger values\n",
    "# have a more consistent interpretation across features\n",
    "X[\"Daily_Screen_Time(hrs)\"] = -X[\"Daily_Screen_Time(hrs)\"]\n",
    "X[\"Days_Without_Social_Media\"] = -X[\"Days_Without_Social_Media\"]\n",
    "\n",
    "# Standardize lifestyle variables\n",
    "scaler_X = StandardScaler()\n",
    "X_z = scaler_X.fit_transform(X)\n",
    "\n",
    "# Apply PCA and keep only the first principal component\n",
    "pca_X = PCA(n_components=1)\n",
    "X_pc1 = pca_X.fit_transform(X_z)\n",
    "\n",
    "# Store the first principal component as a composite lifestyle score\n",
    "df[\"Lifestyle_Score\"] = X_pc1[:, 0]\n",
    "\n",
    "print(\"Lifestyle PCA explained variance ratio:\", pca_X.explained_variance_ratio_)\n",
    "print(\"Lifestyle PCA weights:\", pd.Series(pca_X.components_[0], index=X_cols))\n",
    "\n",
    "\n",
    "# 3. Correlation and visualization\n",
    "# Compute the Pearson correlation between lifestyle and well-being scores\n",
    "corr_uv = df[\"Wellbeing_Score\"].corr(df[\"Lifestyle_Score\"])\n",
    "print(\"Correlation between lifestyle and well-being (PCA-based):\", corr_uv)\n",
    "\n",
    "# Create a scatter plot to visualize the relationship\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(df[\"Lifestyle_Score\"], df[\"Wellbeing_Score\"], alpha=0.4)\n",
    "\n",
    "ax.set_xlabel(\"PCA lifestyle score (higher = healthier)\")\n",
    "ax.set_ylabel(\"PCA well-being score (higher = better)\")\n",
    "ax.set_title(\"Relationship between PCA-based lifestyle and well-being\")\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yifei Pan:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyze how gender and main social-media platform relate to the PCA-based Wellbeing_Score.\n",
    "Steps:\n",
    "1. Encode Gender and Social_Media_Platform as numeric codes.\n",
    "2. Compute Pearson correlation between each code and Wellbeing_Score.\n",
    "3. Visualize both relationships using scatter plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "gender_map = {\"Female\": 1, \"Male\": 0}\n",
    "df[\"Gender_code\"] = df[\"Gender\"].map(gender_map)\n",
    "\n",
    "platform_map = {\n",
    "    \"Facebook\": 0,\n",
    "    \"LinkedIn\": 1,\n",
    "    \"Linkedln\": 1,\n",
    "    \"YouTube\": 2,\n",
    "    \"TikTok\": 3,\n",
    "    \"X (Twitter)\": 4,\n",
    "    \"X\": 4\n",
    "}\n",
    "df[\"Platform_code\"] = df[\"Social_Media_Platform\"].map(platform_map)\n",
    "\n",
    "# 1. Compute correlations with the PCA-based Wellbeing_Score\n",
    "corr_gender = df[\"Gender_code\"].corr(df[\"Wellbeing_Score\"])\n",
    "corr_platform = df[\"Platform_code\"].corr(df[\"Wellbeing_Score\"])\n",
    "\n",
    "print(\"Correlation(Gender_code, Wellbeing_Score):\", corr_gender)\n",
    "print(\"Correlation(Platform_code, Wellbeing_Score):\", corr_platform)\n",
    "\n",
    "# 2. Visualize relationships using scatter plots\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n",
    "\n",
    "#Plot 1: Gender vs Wellbeing_Score\n",
    "axes[0].scatter(df[\"Gender_code\"], df[\"Wellbeing_Score\"], alpha=0.5)\n",
    "axes[0].set_xticks([0, 1])\n",
    "axes[0].set_xticklabels([\"Male\", \"Female\"])\n",
    "axes[0].set_xlabel(\"Gender code (0=Male, 1=Female)\")\n",
    "axes[0].set_ylabel(\"Well-being score\")\n",
    "axes[0].set_title(\"Gender vs PCA well-being score\")\n",
    "\n",
    "#Plot 2: Platform vs Wellbeing_Score\n",
    "axes[1].scatter(df[\"Platform_code\"], df[\"Wellbeing_Score\"], alpha=0.5)\n",
    "axes[1].set_xticks([0, 1, 2, 3, 4])\n",
    "axes[1].set_xticklabels([\"Facebook\", \"LinkedIn\", \"YouTube\", \"TikTok\", \"X\"])\n",
    "axes[1].set_xlabel(\"Platform code\")\n",
    "axes[1].set_ylabel(\"Well-being score\")\n",
    "axes[1].set_title(\"Platform vs PCA well-being score\")\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ruanyiyang Sun:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. The Naive Bayes model provides a reasonable baseline, but its misclassifications are concentrated in intermediate ranges (e.g., sleep 5–6, stress 6–7), reflecting its inability to capture nonlinear relationships among variables.\n",
    "2. These error patterns indicate that the model is sensitive to borderline cases, and its performance is constrained by the independence assumption."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ===============================================================\n",
    "#        Naive Bayes Model Development & Error Analysis\n",
    "# ===============================================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import (accuracy_score, precision_score, recall_score, \n",
    "                           f1_score, confusion_matrix, classification_report,\n",
    "                           roc_curve, auc, precision_recall_curve)\n",
    "print(\"=\"*60)\n",
    "print(\"NAIVE BAYES MODEL DEVELOPMENT AND ERROR PATTERN ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# -------------------------- 1. Data Preparation and Preprocessing --------------------------\n",
    "print(\"\\n1. DATA PREPARATION AND PREPROCESSING\")\n",
    "print(\"-\"*40)\n",
    "df = pd.read_csv(\"Mental_Health_and_Social_Media_Balance_Dataset.csv\")\n",
    "df[\"mental_health_risk\"] = (df[\"Happiness_Index(1-10)\"] <= 6).astype(int)    # Create target variable\n",
    "features = [\n",
    "    \"Age\", \"Gender\", \"Daily_Screen_Time(hrs)\", \"Sleep_Quality(1-10)\",\n",
    "    \"Stress_Level(1-10)\", \"Days_Without_Social_Media\", \"Exercise_Frequency(week)\",\n",
    "    \"Social_Media_Platform\"\n",
    "]     # Select features\n",
    "X = df[features]\n",
    "y = df[\"mental_health_risk\"]\n",
    "print(f\"Number of features: {len(features)}\")\n",
    "print(f\"Target variable distribution: High risk {y.sum()} ({y.sum()/len(y)*100:.1f}%), Low risk {len(y)-y.sum()} ({(len(y)-y.sum())/len(y)*100:.1f}%)\")\n",
    "# Encode categorical variables\n",
    "for col in [\"Gender\", \"Social_Media_Platform\"]:\n",
    "    if col in X.columns:\n",
    "        le = LabelEncoder()\n",
    "        X[col] = le.fit_transform(X[col].astype(str))\n",
    "# Standardize numerical features\n",
    "scaler = StandardScaler()\n",
    "numeric_cols = [\"Age\", \"Daily_Screen_Time(hrs)\", \"Sleep_Quality(1-10)\",\n",
    "                \"Stress_Level(1-10)\", \"Days_Without_Social_Media\", \"Exercise_Frequency(week)\"]\n",
    "numeric_cols = [col for col in numeric_cols if col in X.columns]\n",
    "if numeric_cols:\n",
    "    X[numeric_cols] = scaler.fit_transform(X[numeric_cols])\n",
    "\n",
    "# -------------------------- 2. Train Naive Bayes Model --------------------------\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, stratify=y, random_state=42\n",
    ")\n",
    "# Split into training and testing sets\n",
    "print(f\"Training set: {X_train.shape[0]} samples\")\n",
    "print(f\"Testing set: {X_test.shape[0]} samples\")\n",
    "print(f\"Training set class distribution: High risk {y_train.sum()} ({y_train.sum()/len(y_train)*100:.1f}%), \"\n",
    "      f\"Low risk {len(y_train)-y_train.sum()} ({(len(y_train)-y_train.sum())/len(y_train)*100:.1f}%)\")\n",
    "nb_model = GaussianNB()    # Create and train Naive Bayes model\n",
    "nb_model.fit(X_train, y_train)\n",
    "\n",
    "# -------------------------- 3. Model Performance Evaluation --------------------------\n",
    "y_pred = nb_model.predict(X_test)    # Predictions\n",
    "y_pred_proba = nb_model.predict_proba(X_test)[:, 1]\n",
    "accuracy = accuracy_score(y_test, y_pred)    # Calculate performance metrics\n",
    "precision = precision_score(y_test, y_pred, zero_division=0)\n",
    "recall = recall_score(y_test, y_pred, zero_division=0)\n",
    "f1 = f1_score(y_test, y_pred, zero_division=0)\n",
    "print(\"PERFORMANCE METRICS:\")\n",
    "print(f\"Accuracy:        {accuracy:.4f}\")\n",
    "print(f\"Precision:       {precision:.4f}\")\n",
    "print(f\"Recall:          {recall:.4f}\")\n",
    "print(f\"F1 Score:        {f1:.4f}\")\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "print(f\"\\nCONFUSION MATRIX:\")\n",
    "print(f\"              Predicted High  Predicted Low\")\n",
    "print(f\"Actual High   {tp:12d}    {fn:12d}\")\n",
    "print(f\"Actual Low    {fp:12d}    {tn:12d}\")\n",
    "print(f\"\\nDETAILED METRICS:\")\n",
    "print(f\"True Positives (TP): {tp} - Correctly predicted high risk\")\n",
    "print(f\"True Negatives (TN): {tn} - Correctly predicted low risk\")\n",
    "print(f\"False Positives (FP): {fp} - Incorrectly predicted as high risk\")\n",
    "print(f\"False Negatives (FN): {fn} - Incorrectly predicted as low risk\")\n",
    "# Classification report\n",
    "print(\"\\nCLASSIFICATION REPORT:\")\n",
    "print(classification_report(y_test, y_pred, target_names=['Low Risk', 'High Risk']))\n",
    "\n",
    "# -------------------------- 4. Model Visualization Analysis --------------------------\n",
    "fig = plt.figure(figsize=(15, 10))   # Create visualization charts\n",
    "fig.suptitle('Naive Bayes Model Performance and Error Analysis', fontsize=16, fontweight='bold')\n",
    "ax1 = plt.subplot(2, 3, 1)    # 4.1 Confusion matrix heatmap\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['Predicted Low', 'Predicted High'],\n",
    "            yticklabels=['Actual Low', 'Actual High'], ax=ax1)\n",
    "ax1.set_title('Confusion Matrix')\n",
    "ax1.set_ylabel('True Label')\n",
    "ax1.set_xlabel('Predicted Label')\n",
    "ax2 = plt.subplot(2, 3, 2)   # 4.2 ROC curve\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "ax2.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC Curve (AUC = {roc_auc:.3f})')\n",
    "ax2.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random Classifier')\n",
    "ax2.set_xlim([0.0, 1.0])\n",
    "ax2.set_ylim([0.0, 1.05])\n",
    "ax2.set_xlabel('False Positive Rate')\n",
    "ax2.set_ylabel('True Positive Rate')\n",
    "ax2.set_title('ROC Curve')\n",
    "ax2.legend(loc=\"lower right\")\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax3 = plt.subplot(2, 3, 3)   # 4.3 Precision-Recall curve\n",
    "precision_vals, recall_vals, _ = precision_recall_curve(y_test, y_pred_proba)\n",
    "pr_auc = auc(recall_vals, precision_vals)\n",
    "ax3.plot(recall_vals, precision_vals, color='darkgreen', lw=2, label=f'PR Curve (AUC = {pr_auc:.3f})')\n",
    "baseline = len(y_test[y_test==1]) / len(y_test)\n",
    "ax3.axhline(y=baseline, color='navy', lw=2, linestyle='--', label=f'Baseline (Positive Rate={baseline:.2f})')\n",
    "ax3.set_xlim([0.0, 1.0])\n",
    "ax3.set_ylim([0.0, 1.05])\n",
    "ax3.set_xlabel('Recall')\n",
    "ax3.set_ylabel('Precision')\n",
    "ax3.set_title('Precision-Recall Curve')\n",
    "ax3.legend(loc=\"upper right\")\n",
    "ax3.grid(True, alpha=0.3)\n",
    "ax4 = plt.subplot(2, 3, 4)     # 4.4 Performance metrics comparison\n",
    "metrics = ['Accuracy', 'Precision', 'Recall', 'F1 Score']\n",
    "values = [accuracy, precision, recall, f1]\n",
    "colors = ['skyblue', 'lightgreen', 'lightcoral', 'gold']\n",
    "bars = ax4.bar(metrics, values, color=colors, alpha=0.8)\n",
    "ax4.set_ylabel('Score')\n",
    "ax4.set_title('Performance Metrics Comparison')\n",
    "ax4.set_ylim([0, 1.1])\n",
    "for bar, value in zip(bars, values):     # Add value labels on bars\n",
    "    height = bar.get_height()\n",
    "    ax4.text(bar.get_x() + bar.get_width()/2., height + 0.02,\n",
    "            f'{value:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "ax5 = plt.subplot(2, 3, 5)    # 4.5 Error type distribution\n",
    "error_types = ['Correct', 'False Positive', 'False Negative']\n",
    "error_counts = [tp+tn, fp, fn]\n",
    "error_colors = ['lightgreen', 'lightcoral', 'orange']\n",
    "ax5.pie(error_counts, labels=error_types, colors=error_colors, autopct='%1.1f%%',\n",
    "        startangle=90, explode=(0.05, 0.05, 0.05))\n",
    "ax5.set_title('Prediction Results Distribution')\n",
    "ax6 = plt.subplot(2, 3, 6)    # 4.6 Prediction probability distribution\n",
    "risk_probs = y_pred_proba[y_test == 1]    # Prediction probabilities for high-risk group\n",
    "no_risk_probs = y_pred_proba[y_test == 0]\n",
    "ax6.hist(risk_probs, bins=20, alpha=0.7, label='High Risk Group', color='lightcoral', density=True)    # Use compatible histogram plotting method\n",
    "ax6.hist(no_risk_probs, bins=20, alpha=0.7, label='Low Risk Group', color='lightblue', density=True)\n",
    "ax6.set_xlabel('Predicted High Risk Probability')\n",
    "ax6.set_ylabel('Density')\n",
    "ax6.set_title('Prediction Probability Distribution')\n",
    "ax6.legend()\n",
    "ax6.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# -------------------------- 5. Error Pattern Analysis --------------------------\n",
    "print(\"\\n5. ERROR PATTERN DEEP ANALYSIS\")\n",
    "print(\"-\"*40)\n",
    "# Create error analysis dataset\n",
    "test_results = pd.DataFrame(X_test, columns=X.columns)\n",
    "test_results['Actual Label'] = y_test.values\n",
    "test_results['Predicted Label'] = y_pred\n",
    "test_results['Prediction Probability'] = y_pred_proba\n",
    "test_results['Prediction Correct'] = (y_test.values == y_pred)\n",
    "# Identify error types\n",
    "test_results['Error Type'] = 'Correct'\n",
    "test_results.loc[(test_results['Actual Label'] == 0) & (test_results['Predicted Label'] == 1), 'Error Type'] = 'False Positive'\n",
    "test_results.loc[(test_results['Actual Label'] == 1) & (test_results['Predicted Label'] == 0), 'Error Type'] = 'False Negative'\n",
    "print(f\"ERROR ANALYSIS:\")\n",
    "print(f\"  Correct Predictions: {len(test_results[test_results['Prediction Correct']])} ({len(test_results[test_results['Prediction Correct']])/len(test_results)*100:.1f}%)\")\n",
    "print(f\"  False Positives: {len(test_results[test_results['Error Type']=='False Positive'])} ({len(test_results[test_results['Error Type']=='False Positive'])/len(test_results)*100:.1f}%)\")\n",
    "print(f\"  False Negatives: {len(test_results[test_results['Error Type']=='False Negative'])} ({len(test_results[test_results['Error Type']=='False Negative'])/len(test_results)*100:.1f}%)\")\n",
    "# Analyze error sample features\n",
    "print(\"\\nERROR SAMPLE FEATURE ANALYSIS:\")\n",
    "# Analyze False Positive sample features\n",
    "fp_samples = test_results[test_results['Error Type'] == 'False Positive']\n",
    "if len(fp_samples) > 0:\n",
    "    print(\"\\nFALSE POSITIVE SAMPLES (Actually Low Risk but Predicted as High Risk):\")\n",
    "    print(f\"  Sample Count: {len(fp_samples)}\")\n",
    "    # Analyze key features\n",
    "    key_features_to_check = ['Daily_Screen_Time(hrs)', 'Stress_Level(1-10)', 'Sleep_Quality(1-10)']\n",
    "    for feature in key_features_to_check:\n",
    "        if feature in fp_samples.columns:\n",
    "            avg_value = fp_samples[feature].mean()\n",
    "            print(f\"  {feature}: Average = {avg_value:.2f}\")\n",
    "# Analyze False Negative sample features\n",
    "fn_samples = test_results[test_results['Error Type'] == 'False Negative']\n",
    "if len(fn_samples) > 0:\n",
    "    print(\"\\nFALSE NEGATIVE SAMPLES (Actually High Risk but Predicted as Low Risk):\")\n",
    "    print(f\"  Sample Count: {len(fn_samples)}\")\n",
    "    for feature in key_features_to_check:\n",
    "        if feature in fn_samples.columns:\n",
    "            avg_value = fn_samples[feature].mean()\n",
    "            print(f\"  {feature}: Average = {avg_value:.2f}\")\n",
    "\n",
    "# -------------------------- 6. Model Limitations Analysis --------------------------\n",
    "print(\"\\n6. NAIVE BAYES MODEL LIMITATIONS ANALYSIS\")\n",
    "print(\"-\"*40)\n",
    "print(\"\\nNaive Bayes Model Assumptions and Limitations:\")\n",
    "print(\"=\"*60)\n",
    "print(\"1. Feature Independence Assumption:\")\n",
    "print(\"   • Naive Bayes assumes all features are independent\")\n",
    "print(\"   • In reality, screen time, sleep quality, and stress levels are often correlated\")\n",
    "print(\"   • This may affect the model's ability to capture complex patterns\")\n",
    "print(\"\\n2. Gaussian Distribution Assumption:\")\n",
    "print(\"   • Gaussian Naive Bayes assumes features follow normal distribution\")\n",
    "print(\"   • Real-world data may not meet this assumption\")\n",
    "print(\"   • May require feature transformation or other variants\")\n",
    "print(\"\\n3. Specific Limitations in Mental Health Prediction:\")\n",
    "print(\"   • Mental health is a complex, multi-factor issue\")\n",
    "print(\"   • Single features may not be sufficient for accurate prediction\")\n",
    "print(\"   • Feature interactions are important but Naive Bayes may not capture them well\")\n",
    "print(\"\\n4. Limitations Found from Error Analysis:\")\n",
    "print(\"   • False Positives: Model may overemphasize screen time, ignoring other factors\")\n",
    "print(\"   • False Negatives: Model may miss cases with moderate screen time but significant other risk factors\")\n",
    "print(\"   • May not accurately classify borderline cases\")\n",
    "\n",
    "# -------------------------- 7. Summary --------------------------\n",
    "print(\"\\n7. SUMMARY\")\n",
    "print(\"-\"*40)\n",
    "performance_results = {\n",
    "    'Model': 'GaussianNaiveBayes',\n",
    "    'Accuracy': accuracy,\n",
    "    'Precision': precision,\n",
    "    'Recall': recall,\n",
    "    'F1_Score': f1,\n",
    "    'ROC_AUC': roc_auc,\n",
    "    'PR_AUC': pr_auc,\n",
    "    'True_Positives': int(tp),\n",
    "    'True_Negatives': int(tn),\n",
    "    'False_Positives': int(fp),\n",
    "    'False_Negatives': int(fn),\n",
    "    'Training_Samples': X_train.shape[0],\n",
    "    'Testing_Samples': X_test.shape[0],\n",
    "    'Feature_Count': X_train.shape[1]\n",
    "}\n",
    "performance_df = pd.DataFrame([performance_results])\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"NAIVE BAYES MODEL DEVELOPMENT AND ANALYSIS COMPLETED!\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nMAIN FINDINGS SUMMARY:\")\n",
    "print(\"-\"*40)\n",
    "print(f\"1. Model Accuracy: {accuracy:.3f}\")\n",
    "print(f\"2. Main Error Types:\")\n",
    "print(f\"   • False Positives: {fp} cases (misclassified low risk as high risk)\")\n",
    "print(f\"   • False Negatives: {fn} cases (misclassified high risk as low risk)\")\n",
    "print(f\"3. Key Influencing Factors: Screen time, stress level, sleep quality\")\n",
    "print(f\"4. Suggested Improvements: Consider feature correlations, add interaction terms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tianqi Yin:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "from scipy import stats\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import Lasso, LassoCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Mental_Health_and_Social_Media_Balance_Dataset.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna()\n",
    "df = df.drop(columns = ['User_ID']) #user ID is not useful\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = 'Happiness_Index(1-10)' # Define happiness index as target variable\n",
    "\n",
    "# Split data: X = features, y = target\n",
    "X = df.drop(columns = [target]) \n",
    "y = df[target] \n",
    "\n",
    "# Identify categorical columns\n",
    "categorical_cols = X.select_dtypes(include=['object']).columns\n",
    "\n",
    "# One-hot encode categorical variables if any exist\n",
    "if len(categorical_cols) > 0:\n",
    "    X = pd.get_dummies(X, drop_first=True) # Convert categories to binary columns\n",
    "    print(f\"str_variable: {list(categorical_cols)}\")\n",
    "    print(f\"num_variable: {X.shape[1]}\")\n",
    "# Split data into training and testing sets (90% train, 10% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.1, random_state = 42)# test_size=0.1: Use 10% of data for testing, random_state=42: Ensures reproducible splits\n",
    "print(f\"train: {X_train.shape}, test: {X_test.shape}\")\n",
    "\n",
    "# Standardize features to have mean=0 and variance=1\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train) # Fit on train data and transform it\n",
    "X_test_scaled = scaler.transform(X_test) # Transform test data using same scaling parameters\n",
    "\n",
    "# Store original feature names for later reference\n",
    "feature_names = X.columns.tolist()\n",
    "feature_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check standardization results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training set after standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Mean: [{np.mean(X_train_scaled, axis=0).min():.4f}, {np.mean(X_train_scaled, axis=0).max():.4f}] (should be near 0)\")\n",
    "print(f\"Std: [{np.std(X_train_scaled, axis=0).min():.4f}, {np.std(X_train_scaled, axis=0).max():.4f}] (should be near 1)\")\n",
    "print(f\"Min: {np.min(X_train_scaled, axis=0).min():.4f}, Max: {np.max(X_train_scaled, axis=0).max():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test set after standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Mean: [{np.mean(X_test_scaled, axis=0).min():.4f}, {np.mean(X_test_scaled, axis=0).max():.4f}]\")\n",
    "print(f\"Std: [{np.std(X_test_scaled, axis=0).min():.4f}, {np.std(X_test_scaled, axis=0).max():.4f}]\")\n",
    "print(f\"Min: {np.min(X_test_scaled, axis=0).min():.4f}, Max: {np.max(X_test_scaled, axis=0).max():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test set statistics within acceptable ranges\\\n",
    "StandardScaler Validation: PASSED Test set statistics within acceptable ranges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Approach 1: Using LASSO for feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create range of regularization strengths (alpha values)\n",
    "alphas = np.logspace(-4, 2, 100) # 100 values from 10^-4 to 10^2 (logarithmic spacing)\n",
    "\n",
    "# Track how coefficients change with different alpha values\n",
    "coef_path = []\n",
    "for alpha in alphas:\n",
    "    # Train Lasso model with current alpha\n",
    "    lasso = Lasso(alpha=alpha, max_iter=10000, random_state=42)\n",
    "    lasso.fit(X_train_scaled, y_train)\n",
    "    coef_path.append(lasso.coef_) # Store coefficients for this alpha\n",
    "coef_path = np.array(coef_path) # Convert to numpy array (100 alphas × n_features)\n",
    "\n",
    "# Use cross-validation to find optimal alpha automatically\n",
    "lasso_cv = LassoCV(alphas=np.logspace(-4, 2, 100), # Same alpha range to search\n",
    "                   cv=5,  # 5-fold cross-validation\n",
    "                   max_iter=10000,  # Ensure convergence\n",
    "                   random_state=42)  # Reproducibility\n",
    "lasso_cv.fit(X_train_scaled, y_train) # Train with CV to find best alpha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize: Qualitative judgment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regularization Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure\n",
    "for i in range(coef_path.shape[1]):  # Loop over each feature (axis 1)\n",
    "    plt.plot(np.log10(alphas), coef_path[:, i], label=feature_names[i])\n",
    "\n",
    "# Plot a vertical line for the best alpha found by LassoCV\n",
    "plt.axvline(np.log10(lasso_cv.alpha_), linestyle=\"--\", color=\"k\", label=\"Best alpha (from LassoCV)\")\n",
    "plt.xlabel('log(alpha)')\n",
    "plt.ylabel('Coefficients')\n",
    "plt.title('Lasso Regularization Path')\n",
    "plt.legend(loc='best', bbox_to_anchor=(1, 1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Sort features by absolute coefficient value (most to least important)\n",
    "sorted_idx = np.argsort(np.abs(lasso_cv.coef_))[::-1]\n",
    "sorted_features = [feature_names[i] for i in sorted_idx] \n",
    "sorted_coefs = lasso_cv.coef_[sorted_idx] # Corresponding coefficient values\n",
    "\n",
    "bars = plt.bar(range(len(sorted_features)), sorted_coefs) # X positions: feature indices\n",
    "plt.axhline(y=0, color='black', linestyle='-', linewidth=0.5)\n",
    "plt.xlabel('Features')\n",
    "plt.ylabel('Coefficient Value')\n",
    "plt.title('LASSO Coefficients (Absolute Value Sorted)')\n",
    "plt.xticks(range(len(sorted_features)), sorted_features, rotation=90)\n",
    "\n",
    "# Color bars based on coefficient sign\n",
    "for i, bar in enumerate(bars):\n",
    "    if sorted_coefs[i] > 0:\n",
    "        bar.set_color('blue')\n",
    "    else:\n",
    "        bar.set_color('red')\n",
    "\n",
    "# Adjust layout to prevent label cutoff\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From these two figures, we can see that Stress_Level(1-10), Sleep_Quality(1-10), and Daily_Screen_Time(hrs) are three important features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quantitative judgement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_sellect = {}\n",
    "for i in range(len(feature_names)):\n",
    "    feature_sellect[feature_names[i]] = lasso_cv.coef_[i]\n",
    "\n",
    "good_feature = []\n",
    "for key, value in feature_sellect.items():\n",
    "    if abs(value) > 0.1:\n",
    "        good_feature.append(key)\n",
    "good_feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore, the three good fearture sellected by LASSO are Stress_Level(1-10), Sleep_Quality(1-10), and Daily_Screen_Time(hrs) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Approach 2: Using LDA to sellect feature\n",
    "- LDA needs clear class labels to work. It calculates \"between-class\" and \"within-class\" variance to find features that best separate groups. Continuous y values don't have defined groups, so we must create categories first.\\\n",
    "Since happiness score is ranging from 4 to 10, and according to psychology, <=60% is not feeling well, (from 4 to 7)/from (4 to 10) is approximately 60%, therefore, we consider score from 4 to 7 (score <= 7) is low."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def happiness_group(score):\n",
    "    if score <= 7:\n",
    "        return 'low'\n",
    "    elif score <= 9:\n",
    "        return 'medium'\n",
    "    else:\n",
    "        return 'high'\n",
    "y_group = y.map(happiness_group)\n",
    "y_group\n",
    "y_train_lda, y_test_lda = train_test_split(y_group, test_size = 0.1, random_state = 42)\n",
    "# Initialize and train Linear Discriminant Analysis (LDA) model\n",
    "lda = LinearDiscriminantAnalysis()\n",
    "lda.fit(X_train_scaled, y_train_lda)\n",
    "\n",
    "# Make predictions on test set\n",
    "y_predict_lda = lda.predict(X_test_scaled) # Class predictions (discrete labels)\n",
    "y_predict_proba_lda = lda.predict_proba(X_test_scaled) # Probability predictions for each class\n",
    "\n",
    "# Calculate and display model accuracy\n",
    "accuracy = accuracy_score(y_test_lda, y_predict_lda)# Compare predicted vs actual labels \n",
    "print(f\"accuracy: {accuracy:.3f}\")\n",
    "\n",
    "# Generate detailed classification report\n",
    "classes = classification_report(y_test_lda, y_predict_lda, zero_division = 0) # classification_report provides precision, recall, f1-score for each class, zero_division=0 handles cases where no positive predictions are made for a class\n",
    "print(classes)\n",
    "# Create DataFrame to display LDA discriminant function coefficients\n",
    "coef_df = pd.DataFrame(\n",
    "    lda.coef_, # Coefficient matrix from LDA (shape: n_classes-1 × n_features)\n",
    "    index=[f\"function{i+1}\" for i in range(lda.coef_.shape[0])],\n",
    "    columns=X.columns\n",
    ")\n",
    "print(coef_df.round(3))\n",
    "print(lda.intercept_.round(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate LDA projections\n",
    "X_train_lda = lda.transform(X_train_scaled)\n",
    "X_test_lda = lda.transform(X_test_scaled)\n",
    "\n",
    "print(f\"Training set: {len(X_train_lda)} points\")\n",
    "print(f\"Test set: {len(X_test_lda)} points\")\n",
    "\n",
    "categories = np.unique(y_train_lda)\n",
    "colors = ['red', 'green', 'blue']\n",
    "\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "# Subplot 1: Training set with class counts\n",
    "plt.subplot(1, 2, 1)\n",
    "for i, category in enumerate(categories):\n",
    "    mask = y_train_lda == category\n",
    "    count = np.sum(mask)\n",
    "    plt.scatter(X_train_lda[mask, 0], X_train_lda[mask, 1], \n",
    "               c=colors[i], label=f'{category} (n={count})', \n",
    "               alpha=0.7, s=50, edgecolors='black', linewidth=0.5)\n",
    "    \n",
    "plt.xlabel('function1 (99.2% variance)')\n",
    "plt.ylabel('function2 (0.76% variance)')\n",
    "plt.title(f'Training Set - {len(X_train_lda)} total points')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Subplot 2: Test set with accuracy breakdown\n",
    "plt.subplot(1, 2, 2)\n",
    "\n",
    "# Calculate class-wise counts\n",
    "class_counts = {}\n",
    "for category in categories:\n",
    "    mask = y_test_lda == category\n",
    "    class_counts[category] = np.sum(mask)\n",
    "    plt.scatter(X_test_lda[mask, 0], X_test_lda[mask, 1], \n",
    "               c=colors[np.where(categories == category)[0][0]], \n",
    "               label=f'{category} (n={np.sum(mask)})',\n",
    "               alpha=0.7, s=50, edgecolors='black', linewidth=0.5)\n",
    "\n",
    "# Highlight wrong predictions\n",
    "wrong_predictions = y_predict_lda != y_test_lda\n",
    "wrong_count = np.sum(wrong_predictions)\n",
    "if wrong_count > 0:\n",
    "    plt.scatter(X_test_lda[wrong_predictions, 0], \n",
    "               X_test_lda[wrong_predictions, 1],\n",
    "               c='black', marker='x', s=150, \n",
    "               label=f'Wrong ({wrong_count} points)', \n",
    "               linewidth=2, alpha=1.0)\n",
    "\n",
    "plt.xlabel('function1')\n",
    "plt.ylabel('function2')\n",
    "plt.title(f'Test Set - {len(X_test_lda)} points, Accuracy: {accuracy:.3f}')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Additional analysis\n",
    "print(\"\\n=== Detailed Test Set Analysis ===\")\n",
    "print(f\"Total test points: {len(X_test_lda)}\")\n",
    "print(f\"Accuracy: {accuracy:.3f}\")\n",
    "print(f\"Number wrong: {wrong_count}\")\n",
    "print(f\"Number correct: {len(X_test_lda) - wrong_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting SVM Decision Boundaries on LDA-Projected Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Prepare data (use LDA-projected 2D data)\n",
    "# X_train_lda, X_test_lda are already computed from LDA\n",
    "# y_train_lda, y_test_lda are your categorical labels\n",
    "\n",
    "# 2. Convert string labels to numeric for SVM\n",
    "le = LabelEncoder()\n",
    "y_train_numeric = le.fit_transform(y_train_lda)\n",
    "y_test_numeric = le.transform(y_test_lda)\n",
    "\n",
    "# 3. Train SVM with RBF kernel on LDA space\n",
    "svm = SVC(kernel='rbf', C=1.0, gamma='scale', probability=True, random_state=42)\n",
    "svm.fit(X_train_lda, y_train_numeric)\n",
    "\n",
    "# 4. Create mesh grid for decision boundary visualization\n",
    "x_min, x_max = X_train_lda[:, 0].min() - 0.5, X_train_lda[:, 0].max() + 0.5\n",
    "y_min, y_max = X_train_lda[:, 1].min() - 0.5, X_train_lda[:, 1].max() + 0.5\n",
    "xx, yy = np.meshgrid(np.linspace(x_min, x_max, 300),\n",
    "                     np.linspace(y_min, y_max, 300))\n",
    "\n",
    "# 5. Predict on mesh grid\n",
    "Z = svm.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "# 6. Plot decision boundaries\n",
    "plt.figure(figsize=(12, 10))\n",
    "\n",
    "# Background: decision regions\n",
    "cmap_background = ListedColormap(['#FFCCCC', '#CCFFCC', '#CCCCFF'])\n",
    "plt.contourf(xx, yy, Z, alpha=0.3, cmap=cmap_background)\n",
    "\n",
    "# Decision boundaries (contours)\n",
    "plt.contour(xx, yy, Z, colors='black', linewidths=0.5, alpha=0.5)\n",
    "\n",
    "# 7. Plot test data points with true labels\n",
    "colors = ['red', 'green', 'blue']\n",
    "for i in range(3):  # 3 classes\n",
    "    mask = y_test_numeric == i\n",
    "    plt.scatter(X_test_lda[mask, 0], X_test_lda[mask, 1],\n",
    "               c=colors[i], marker='o', s=80,\n",
    "               edgecolors='black', linewidth=0.8,\n",
    "               label=f'{le.inverse_transform([i])[0]} (True)',\n",
    "               alpha=0.8)\n",
    "\n",
    "# 8. Calculate and display accuracy\n",
    "y_test_pred_svm = svm.predict(X_test_lda)\n",
    "svm_accuracy = np.mean(y_test_pred_svm == y_test_numeric)\n",
    "\n",
    "plt.xlabel('LD1 (99.2% variance)', fontsize=12)\n",
    "plt.ylabel('LD2 (0.76% variance)', fontsize=12)\n",
    "plt.title(f'SVM Decision Boundaries on LDA Space\\nTest Accuracy: LDA={accuracy:.3f}, SVM={svm_accuracy:.3f}', \n",
    "          fontsize=14, fontweight='bold')\n",
    "plt.legend(loc='upper right')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# 9. Add informative text box\n",
    "textstr = '\\n'.join((\n",
    "    f'Total test samples: {len(y_test_lda)}',\n",
    "    f'SVM kernel: RBF',\n",
    "    f'LDA accuracy: {accuracy:.3f}',\n",
    "    f'SVM accuracy: {svm_accuracy:.3f}'))\n",
    "\n",
    "props = dict(boxstyle='round', facecolor='wheat', alpha=0.8)\n",
    "plt.text(0.02, 0.98, textstr, transform=plt.gca().transAxes,\n",
    "         fontsize=10, verticalalignment='top', bbox=props)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 10. Print comparison summary\n",
    "print(\"=== Model Performance Summary ===\")\n",
    "print(f\"LDA Classification Accuracy: {accuracy:.3f}\")\n",
    "print(f\"SVM on LDA space Accuracy: {svm_accuracy:.3f}\")\n",
    "print(f\"Improvement: {svm_accuracy - accuracy:+.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVM Probability Contours on LDA Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data - convert string labels to numeric\n",
    "le = LabelEncoder()\n",
    "y_train_numeric = le.fit_transform(y_train_lda)\n",
    "y_test_numeric = le.transform(y_test_lda)\n",
    "\n",
    "# Train SVM with probability estimates\n",
    "svm_prob = SVC(kernel='rbf', C=1.0, gamma='scale', \n",
    "               probability=True, random_state=42)\n",
    "svm_prob.fit(X_train_lda, y_train_numeric)\n",
    "\n",
    "# Create grid for visualization\n",
    "x_min, x_max = X_train_lda[:, 0].min() - 0.5, X_train_lda[:, 0].max() + 0.5\n",
    "y_min, y_max = X_train_lda[:, 1].min() - 0.5, X_train_lda[:, 1].max() + 0.5\n",
    "xx, yy = np.meshgrid(np.linspace(x_min, x_max, 200),\n",
    "                     np.linspace(y_min, y_max, 200))\n",
    "\n",
    "# Get probability predictions over the grid\n",
    "Z_proba = svm_prob.predict_proba(np.c_[xx.ravel(), yy.ravel()])\n",
    "\n",
    "# Plot contour maps\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "categories = le.classes_  # Should be ['high', 'low', 'medium'] or similar\n",
    "\n",
    "for i, (ax, category) in enumerate(zip(axes, categories)):\n",
    "    # Probability for current class\n",
    "    Z_i = Z_proba[:, i].reshape(xx.shape)\n",
    "    \n",
    "    # Plot probability contours\n",
    "    contour = ax.contourf(xx, yy, Z_i, levels=20, cmap='RdYlBu', alpha=0.8)\n",
    "    \n",
    "    # Plot test data points\n",
    "    colors = ['red', 'green', 'blue']\n",
    "    for j in range(3):\n",
    "        mask = y_test_numeric == j\n",
    "        ax.scatter(X_test_lda[mask, 0], X_test_lda[mask, 1],\n",
    "                  c=colors[j], edgecolors='black', s=50, alpha=0.7,\n",
    "                  label=f'{le.inverse_transform([j])[0]}')\n",
    "    \n",
    "    # Add decision boundary (probability = 0.5)\n",
    "    cs = ax.contour(xx, yy, Z_i, levels=[0.5], colors='black', linewidths=2)\n",
    "    \n",
    "    ax.set_xlabel('function1')\n",
    "    ax.set_ylabel('function2')\n",
    "    ax.set_title(f'P({category} | x)')\n",
    "    ax.legend(loc='upper right')\n",
    "    plt.colorbar(contour, ax=ax)\n",
    "\n",
    "plt.suptitle('SVM Classification Probabilities with Contour Lines\\nRed=High Probability, Blue=Low Probability', \n",
    "             fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How much these functions contribute to predicting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coef_rank = np.linalg.matrix_rank(lda.coef_)\n",
    "print(f\"Coefficient matrix rank: {coef_rank}\")\n",
    "\n",
    "if hasattr(lda, 'explained_variance_ratio_'):\n",
    "    print(f\"Explained variance ratio: {lda.explained_variance_ratio_}\")\n",
    "\n",
    "X_cov = np.cov(X_train_scaled.T)\n",
    "print(f\"Feature covariance matrix rank: {np.linalg.matrix_rank(X_cov)}\")\n",
    "\n",
    "print(f\"Number of classes: {len(lda.classes_)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explained variance: [0.9923695 0.0076305]\\\n",
    "function 1: ～99.2% contribution (dominant)\\\n",
    "function 2: ～0.7% contribution (negligible)\\\n",
    "function 3: Redundant (zero contribution, linear combination of f1 & f2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize in one dimention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "# Set academic style\n",
    "plt.style.use('default')\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "# Combine data for visualization\n",
    "X_lda_combined = np.vstack([X_train_lda, X_test_lda])\n",
    "y_combined = np.concatenate([y_train_lda, y_test_lda])\n",
    "\n",
    "# Create DataFrame\n",
    "lda_df = pd.DataFrame({\n",
    "    'LD1': X_lda_combined[:, 0],\n",
    "    'LD2': X_lda_combined[:, 1] if X_lda_combined.shape[1] > 1 else np.zeros(len(X_lda_combined)),\n",
    "    'Happiness': y_combined\n",
    "})\n",
    "\n",
    "# Professional color palette (ColorBrewer Set1)\n",
    "colors = ['#E74C3C', '#2ECC71', '#3498DB']  # Red, Green, Blue\n",
    "categories = ['low', 'medium', 'high']\n",
    "\n",
    "# Create figure\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4.5))\n",
    "\n",
    "# Subplot 1: function1 Distribution\n",
    "ax1 = axes[0]\n",
    "for i, category in enumerate(categories):\n",
    "    data = lda_df[lda_df['Happiness'] == category]['LD1'].dropna()\n",
    "    if len(data) > 1:\n",
    "        # Gaussian Kernel Density Estimation\n",
    "        kde = stats.gaussian_kde(data, bw_method='scott')\n",
    "        x_range = np.linspace(data.min(), data.max(), 200)\n",
    "        density = kde(x_range)\n",
    "        \n",
    "        ax1.plot(x_range, density, color=colors[i], linewidth=2.5, \n",
    "                label=category.capitalize())\n",
    "        \n",
    "        # Add mean line\n",
    "        mean_val = data.mean()\n",
    "        ax1.axvline(x=mean_val, color=colors[i], linestyle='--', \n",
    "                   alpha=0.6, linewidth=1.2)\n",
    "\n",
    "ax1.set_xlabel('function1 (99.2% explained variance)', fontsize=11)\n",
    "ax1.set_ylabel('Probability Density', fontsize=11)\n",
    "ax1.set_title('First Discriminant Function', fontsize=12, fontweight='bold')\n",
    "ax1.legend(title='Happiness Level', fontsize=9)\n",
    "ax1.grid(True, alpha=0.2)\n",
    "\n",
    "# Subplot 2: function2 Distribution\n",
    "ax2 = axes[1]\n",
    "for i, category in enumerate(categories):\n",
    "    data = lda_df[lda_df['Happiness'] == category]['LD2'].dropna()\n",
    "    if len(data) > 1:\n",
    "        kde = stats.gaussian_kde(data, bw_method='scott')\n",
    "        x_range = np.linspace(data.min(), data.max(), 200)\n",
    "        density = kde(x_range)\n",
    "        \n",
    "        ax2.plot(x_range, density, color=colors[i], linewidth=2.5, \n",
    "                label=category.capitalize())\n",
    "        \n",
    "        mean_val = data.mean()\n",
    "        ax2.axvline(x=mean_val, color=colors[i], linestyle='--', \n",
    "                   alpha=0.6, linewidth=1.2)\n",
    "\n",
    "ax2.set_xlabel('function2 (0.76% explained variance)', fontsize=11)\n",
    "ax2.set_ylabel('Probability Density', fontsize=11)\n",
    "ax2.set_title('Second Discriminant Function', fontsize=12, fontweight='bold')\n",
    "ax2.legend(title='Happiness Level', fontsize=9)\n",
    "ax2.grid(True, alpha=0.2)\n",
    "\n",
    "# Main title\n",
    "plt.suptitle('Probability Density Functions of LDA Discriminant Features', \n",
    "             fontsize=13, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore, function1 is much more important than function2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the absolute value of the coefficient of function1\n",
    "first_coef = np.abs(lda.coef_[0])\n",
    "\n",
    "# use rank() function\n",
    "importance_df = pd.DataFrame({\n",
    "    'feature': X.columns,\n",
    "    'abs_coefficient': first_coef,\n",
    "    'importance_rank': pd.Series(-first_coef).rank(method='min').astype(int)\n",
    "}).sort_values('abs_coefficient', ascending=False)\n",
    "\n",
    "print(importance_df.to_string(index=False))\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By using LDA, the most important features are Stress_Level(1-10) and Sleep_Quality(1-10), which overlap with Lasso "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Boxplot Visualization: Stress Level & Sleep Quality vs Happiness Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize = (12, 5))\n",
    "for i, col in enumerate(['Stress_Level(1-10)', 'Sleep_Quality(1-10)']):\n",
    "    sns.boxplot(x='Happiness_Index(1-10)', y=col, data=df, ax=axes[i])\n",
    "    axes[i].set_title(f'{col} - Happiness')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 Conclusions\n",
    "\n",
    "The combined LASSO and logistic‐regression analyses indicate that a small set of lifestyle variables—particularly screen time and exercise frequency—provides measurable but limited predictive power for wellbeing and binary mental-health status. While L1 regularization helps isolate the most influential predictors and the derived decision boundary offers an interpretable behavioural threshold, overall model performance remains moderate, constrained by linearity and sparse feature design. Future work should incorporate richer behavioural features and more flexible nonlinear models to capture the multidimensional structure of wellbeing more effectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project, we used the Mental Health and Social Media Balance dataset to build an interpretable pipeline that links digital behavior to college students’ well-being. Using PCA, we created latent **Wellbeing_Score** and **Lifestyle_Score** measures that summarize multiple survey items into psychologically meaningful dimensions. These scores revealed a clear positive relationship between healthier lifestyles and better well-being.\n",
    "\n",
    "Through **Project 1**, we showed that Lasso and logistic regression can identify **daily screen time** as the dominant behavioral correlate of well-being and can provide an empirically grounded threshold of roughly **5–6 hours per day**, beyond which the likelihood of being in a good-status group declines. **Project 2** demonstrated that relatively simple classifiers, such as Gaussian Naive Bayes, can reliably detect high-risk students and improve substantially over a majority baseline, especially in terms of recall for the high-risk group. **Project 3** further highlighted the importance of stress, sleep quality, and screen time by showing that these three variables alone allow LDA to separate low-, medium-, and high-happiness groups with about 70% accuracy.\n",
    "\n",
    "Taken together, our findings point to a nuanced view of social media and mental health. The key message is not that students should completely abandon social media, but that **very high daily screen time, combined with poor sleep and high stress, is associated with markedly worse outcomes**. For universities, this suggests that practical interventions might focus on helping students keep daily social-media use below very high levels, improving sleep hygiene, and monitoring stress, rather than enforcing unrealistic zero-use policies. Methodologically, our work illustrates how traditional statistical models and basic machine-learning tools can be combined to produce results that are both predictive and interpretable, providing a useful bridge between computational methods and social-science questions about how young people live and experience their lives online."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yifei Pan:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LASSO regression to predict the PCA-based Wellbeing_Score\n",
    "from three lifestyle variables:\n",
    "  - Daily_Screen_Time(hrs)\n",
    "  - Days_Without_Social_Media\n",
    "  - Exercise_Frequency(week)\n",
    "Steps:\n",
    "1. Split data into training and test sets.\n",
    "2. Standardize predictors.\n",
    "3. Fit a LASSO model and inspect MSE, R², and coefficients.\n",
    "4. Visualize true vs predicted scores.\n",
    "5. Plot fitted relationships for screen time and exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Select predictor matrix X and response vector y\n",
    "X = df.loc[:, [\"Daily_Screen_Time(hrs)\",\n",
    "               \"Days_Without_Social_Media\",\n",
    "               \"Exercise_Frequency(week)\"]]\n",
    "y = df.loc[:, \"Wellbeing_Score\"]\n",
    "\n",
    "# 1. Train–test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# 2. Standardize predictors (important for L1 regularization)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# 3. Fit LASSO model and evaluate performance\n",
    "lasso = Lasso(alpha=0.1, random_state=42)\n",
    "lasso.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predict Wellbeing_Score for the test set\n",
    "y_pred = lasso.predict(X_test_scaled)\n",
    "\n",
    "# Compute evaluation metrics\n",
    "print(\"MSE:\", mean_squared_error(y_test, y_pred))\n",
    "print(\"R^2:\", r2_score(y_test, y_pred))\n",
    "\n",
    "# Report model parameters\n",
    "print(\"Best lambda (alpha):\", lasso.alpha)\n",
    "print(\"Intercept:\", lasso.intercept_)\n",
    "print(\"Coefficients:\")\n",
    "for name, coef in zip(X.columns, lasso.coef_):\n",
    "    print(f\"{name}: {coef:.4f}\")\n",
    "\n",
    "\n",
    "# 4. Plot true vs predicted Wellbeing_Score for the test set\n",
    "plt.figure()\n",
    "plt.scatter(y_test, y_pred, alpha=0.7)\n",
    "plt.xlabel(\"True Wellbeing_Score\")\n",
    "plt.ylabel(\"Predicted Wellbeing_Score\")\n",
    "plt.title(\"Lasso: True vs Predicted Wellbeing\")\n",
    "# 45-degree reference line (perfect predictions)\n",
    "plt.axline((0, 0), slope=1)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# 5a. Partial effect plot for Daily_Screen_Time(hrs)\n",
    "\n",
    "x_name = \"Daily_Screen_Time(hrs)\"\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(df[x_name], df[\"Wellbeing_Score\"],\n",
    "            alpha=0.5, label=\"data\")\n",
    "\n",
    "# Create a grid of screen-time values across the observed range\n",
    "x_grid = np.linspace(df[x_name].min(), df[x_name].max(), 100)\n",
    "\n",
    "# Hold the other two predictors at their sample means\n",
    "mean_days = df[\"Days_Without_Social_Media\"].mean()\n",
    "mean_exer = df[\"Exercise_Frequency(week)\"].mean()\n",
    "\n",
    "# Construct a design matrix for prediction\n",
    "X_grid = pd.DataFrame({\n",
    "    \"Daily_Screen_Time(hrs)\": x_grid,\n",
    "    \"Days_Without_Social_Media\": mean_days,\n",
    "    \"Exercise_Frequency(week)\": mean_exer\n",
    "})\n",
    "\n",
    "# Apply the same scaling and get predicted Wellbeing_Score\n",
    "X_grid_scaled = scaler.transform(X_grid)\n",
    "y_grid_pred = lasso.predict(X_grid_scaled)\n",
    "\n",
    "# Plot fitted LASSO line on top of the scatter plot\n",
    "plt.plot(x_grid, y_grid_pred, linewidth=2, label=\"Lasso fit\")\n",
    "plt.xlabel(\"Daily_Screen_Time(hrs)\")\n",
    "plt.ylabel(\"Wellbeing_Score\")\n",
    "plt.title(\"Screen time vs Wellbeing (Lasso)\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# 5b. Partial effect plot for Exercise_Frequency(week)\n",
    "\n",
    "z_name = \"Exercise_Frequency(week)\"\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(df[z_name], df[\"Wellbeing_Score\"],\n",
    "            alpha=0.5, label=\"data\")\n",
    "\n",
    "# Grid over the range of exercise frequency\n",
    "z_grid = np.linspace(df[z_name].min(), df[z_name].max(), 100)\n",
    "\n",
    "# Hold screen time and days-without-social-media at their means\n",
    "mean_days = df[\"Days_Without_Social_Media\"].mean()\n",
    "mean_time = df[\"Daily_Screen_Time(hrs)\"].mean()\n",
    "\n",
    "Z_grid = pd.DataFrame({\n",
    "    \"Daily_Screen_Time(hrs)\": mean_time,\n",
    "    \"Days_Without_Social_Media\": mean_days,\n",
    "    \"Exercise_Frequency(week)\": z_grid\n",
    "})\n",
    "\n",
    "# Scale and predict over this grid\n",
    "Z_grid_scaled = scaler.transform(Z_grid)\n",
    "y_grid_pred = lasso.predict(Z_grid_scaled)\n",
    "\n",
    "# Plot fitted curve\n",
    "plt.plot(z_grid, y_grid_pred, linewidth=2, label=\"Lasso fit\")\n",
    "plt.xlabel(\"Exercise_Frequency(week)\")\n",
    "plt.ylabel(\"Wellbeing_Score\")\n",
    "plt.title(\"Exercise Frequency vs Wellbeing (Lasso)\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Yifei Pan:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic regression pipeline to predict a binary\n",
    "Good_status label from three lifestyle variables:\n",
    "  - Daily_Screen_Time(hrs)\n",
    "  - Days_Without_Social_Media\n",
    "  - Exercise_Frequency(week)\n",
    "Steps:\n",
    "1. Use cross-validation to compare different L2 strengths (C).\n",
    "2. Construct a binary Good_status label from Wellbeing_Score.\n",
    "3. Split the data into train and test sets and standardize X.\n",
    "4. Use L1 logistic regression for feature selection.\n",
    "5. Refit an L2 logistic regression on selected features.\n",
    "6. Evaluate classification performance and compute the\n",
    "    decision boundary in terms of screen time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    confusion_matrix,\n",
    "    classification_report\n",
    ")\n",
    "\n",
    "# Feature names used in the logistic regression models\n",
    "feature_cols = [\n",
    "    \"Daily_Screen_Time(hrs)\",\n",
    "    \"Days_Without_Social_Media\",\n",
    "    \"Exercise_Frequency(week)\"\n",
    "]\n",
    "\n",
    "# 1. Cross-validation to choose a reasonable L2 regularization strength C\n",
    "# Use the original Wellbeing_Score as a temporary target\n",
    "\n",
    "target_col = \"Wellbeing_Score\"\n",
    "y_tmp = df[target_col]\n",
    "X_tmp = df[feature_cols]\n",
    "\n",
    "# Standardize predictors for logistic regression\n",
    "scaler_tmp = StandardScaler()\n",
    "X_tmp_scaled = scaler_tmp.fit_transform(X_tmp)\n",
    "\n",
    "Cs = [0.01, 0.1, 0.5, 1.0]\n",
    "for C in Cs:\n",
    "    model = LogisticRegression(\n",
    "        penalty=\"l2\",\n",
    "        C=C,\n",
    "        max_iter=2000,\n",
    "        random_state=42\n",
    "    )\n",
    "    scores = cross_val_score(\n",
    "        model,\n",
    "        X_tmp_scaled,\n",
    "        (y_tmp >= y_tmp.median()).astype(int),  # temporary binary label\n",
    "        cv=5,\n",
    "        scoring=\"accuracy\"\n",
    "    )\n",
    "    print(f\"C={C}, mean CV accuracy={scores.mean():.3f}\")\n",
    "\n",
    "# 2. Construct the final binary target: Good_status\n",
    "\n",
    "# Here we define Good_status using the median of Wellbeing_Score\n",
    "median_wellbeing = df[\"Wellbeing_Score\"].median()\n",
    "df[\"Good_status\"] = (df[\"Wellbeing_Score\"] >= median_wellbeing).astype(int)\n",
    "\n",
    "target_col = \"Good_status\"\n",
    "\n",
    "X = df[feature_cols]\n",
    "y = df[target_col]\n",
    "\n",
    "# 3. Train–test split and standardization\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X,\n",
    "    y,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=y  # keep the proportion of Good_status in both sets\n",
    ")\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# 4. L1 logistic regression for feature selection\n",
    "\n",
    "log_reg_l1 = LogisticRegression(\n",
    "    penalty=\"l1\",\n",
    "    C=0.1,\n",
    "    solver=\"liblinear\",  # liblinear supports L1 penalty\n",
    "    max_iter=2000,\n",
    "    random_state=42\n",
    ")\n",
    "log_reg_l1.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Extract L1 coefficients and select non-zero features\n",
    "coef_l1 = log_reg_l1.coef_[0]\n",
    "mask = np.abs(coef_l1) > 1e-6\n",
    "selected_features = [f for f, m in zip(feature_cols, mask) if m]\n",
    "print(\"Selected features:\", selected_features)\n",
    "\n",
    "# Keep only selected features for further modeling\n",
    "X_train_sel_scaled = X_train_scaled[:, mask]\n",
    "X_test_sel_scaled = X_test_scaled[:, mask]\n",
    "\n",
    "# 5. L2 logistic regression on selected features\n",
    "\n",
    "log_reg_l2 = LogisticRegression(\n",
    "    penalty=\"l2\",\n",
    "    C=1.0,\n",
    "    max_iter=2000,\n",
    "    random_state=42\n",
    ")\n",
    "log_reg_l2.fit(X_train_sel_scaled, y_train)\n",
    "\n",
    "# Predict Good_status on the test set\n",
    "y_pred = log_reg_l2.predict(X_test_sel_scaled)\n",
    "y_proba = log_reg_l2.predict_proba(X_test_sel_scaled)[:, 1]\n",
    "\n",
    "print(\"=== Default threshold (≈0.5) ===\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Precision:\", precision_score(y_test, y_pred, zero_division=0))\n",
    "print(\"Recall:\", recall_score(y_test, y_pred, zero_division=0))\n",
    "print(\"F1-score:\", f1_score(y_test, y_pred, zero_division=0))\n",
    "print(\"Confusion matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "print(\"\\nClassification report:\\n\",\n",
    "      classification_report(y_test, y_pred, zero_division=0))\n",
    "\n",
    "# 6. Inspect L2 coefficients and derive a decision boundary for Daily_Screen_Time(hrs) at p = 0.5\n",
    "\n",
    "coef_l2 = log_reg_l2.coef_[0]\n",
    "print(\"\\nL2 coefficients on selected features:\")\n",
    "for name, c in zip(selected_features, coef_l2):\n",
    "    print(f\"{name}: {c:.4f}\")\n",
    "\n",
    "# For simplicity, we derive a decision boundary only for the\n",
    "# case where Daily_Screen_Time(hrs) is among the selected\n",
    "# features and dominates the model.\n",
    "beta0 = log_reg_l2.intercept_[0]\n",
    "\n",
    "# We assume the first selected feature is Daily_Screen_Time(hrs)\n",
    "beta1 = log_reg_l2.coef_[0][0]\n",
    "feature_name = \"Daily_Screen_Time(hrs)\"\n",
    "\n",
    "# Index of this feature in the original feature list (used to\n",
    "# retrieve mean and std from the scaler)\n",
    "idx = feature_cols.index(feature_name)\n",
    "\n",
    "mu = scaler.mean_[idx]\n",
    "sigma = scaler.scale_[idx]\n",
    "\n",
    "# In the standardized space, the decision boundary at p=0.5 is:\n",
    "#   beta0 + beta1 * z = 0  →  z* = -beta0 / beta1\n",
    "z_star = -beta0 / beta1\n",
    "\n",
    "# Convert back to the original screen time scale\n",
    "x_star = mu + z_star * sigma\n",
    "\n",
    "print(f\"\\nDecision boundary for {feature_name} at p=0.5: {x_star:.2f} hours\")\n",
    "\n",
    "if beta1 < 0:\n",
    "    print(\"Screen time BELOW this value → more likely to be in Good_status (y=1)\")\n",
    "    print(\"Screen time ABOVE this value → more likely NOT in Good_status (y=0)\")\n",
    "else:\n",
    "    print(\"Screen time ABOVE this value → more likely to be in Good_status (y=1)\")\n",
    "    print(\"Screen time BELOW this value → more likely NOT in Good_status (y=0)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Future work should introduce models capable of handling complex interactions (such as tree-based models or boosting) and enhance robustness through more refined feature engineering.\n",
    "Additionally, collecting time-series data and behavioral trajectory data will significantly improve predictive depth."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ruanyiyang Sun:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results demonstrate a systematic association between social media usage behaviors and happiness indices, while the predictive ability of Naive Bayes remains limited."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================================================\n",
    "#        XGBoost vs Naive Bayes Model Comparison\n",
    "# ===============================================================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import (accuracy_score, precision_score, recall_score, \n",
    "                           f1_score, confusion_matrix, classification_report,\n",
    "                           roc_curve, auc)\n",
    "print(\"=\"*60)\n",
    "print(\"XGBOOST vs NAIVE BAYES MODEL COMPARISON\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# -------------------------- 1. Data Preparation --------------------------\n",
    "print(\"\\n1. DATA PREPARATION\")\n",
    "print(\"-\"*40)\n",
    "df = pd.read_csv(\"Mental_Health_and_Social_Media_Balance_Dataset.csv\")\n",
    "df[\"mental_health_risk\"] = (df[\"Happiness_Index(1-10)\"] <= 6).astype(int) # Create target variable\n",
    "features = [\n",
    "    \"Age\", \"Gender\", \"Daily_Screen_Time(hrs)\", \"Sleep_Quality(1-10)\",\n",
    "    \"Stress_Level(1-10)\", \"Days_Without_Social_Media\", \"Exercise_Frequency(week)\",\n",
    "    \"Social_Media_Platform\"\n",
    "]  # Select features\n",
    "X = df[features]\n",
    "y = df[\"mental_health_risk\"]\n",
    "print(\"Dataset size: {} samples\".format(df.shape[0]))\n",
    "print(\"Number of features: {}\".format(len(features)))\n",
    "print(\"Target distribution: High risk {} ({:.1f}%), Low risk {} ({:.1f}%)\".format(\n",
    "    y.sum(), y.sum()/len(y)*100, len(y)-y.sum(), (len(y)-y.sum())/len(y)*100))\n",
    "for col in [\"Gender\", \"Social_Media_Platform\"]:    # Encode categorical variables\n",
    "    if col in X.columns:\n",
    "        le = LabelEncoder()\n",
    "        X[col] = le.fit_transform(X[col].astype(str))\n",
    "scaler = StandardScaler() # Standardize numerical features\n",
    "numeric_cols = [\"Age\", \"Daily_Screen_Time(hrs)\", \"Sleep_Quality(1-10)\",\n",
    "                \"Stress_Level(1-10)\", \"Days_Without_Social_Media\", \"Exercise_Frequency(week)\"]\n",
    "numeric_cols = [col for col in numeric_cols if col in X.columns]\n",
    "if numeric_cols:\n",
    "    X[numeric_cols] = scaler.fit_transform(X[numeric_cols])\n",
    "\n",
    "\n",
    "# -------------------------- 2. Train-Test Split --------------------------\n",
    "print(\"\\n2. TRAIN-TEST SPLIT\")\n",
    "print(\"-\"*40)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, stratify=y, random_state=42\n",
    ")\n",
    "print(\"Training set: {} samples\".format(X_train.shape[0]))\n",
    "print(\"Testing set: {} samples\".format(X_test.shape[0]))\n",
    "print(\"Training set class distribution: High risk {} ({:.1f}%), Low risk {} ({:.1f}%)\".format(\n",
    "    y_train.sum(), y_train.sum()/len(y_train)*100, \n",
    "    len(y_train)-y_train.sum(), (len(y_train)-y_train.sum())/len(y_train)*100))\n",
    "\n",
    "# -------------------------- 3. Train Naive Bayes Model --------------------------\n",
    "print(\"\\n3. TRAINING NAIVE BAYES MODEL\")\n",
    "print(\"-\"*40)\n",
    "nb_model = GaussianNB()\n",
    "nb_model.fit(X_train, y_train)\n",
    "nb_y_pred = nb_model.predict(X_test)   # Make predictions\n",
    "nb_y_pred_proba = nb_model.predict_proba(X_test)[:, 1]\n",
    "nb_accuracy = accuracy_score(y_test, nb_y_pred)    # Calculate performance metrics\n",
    "nb_precision = precision_score(y_test, nb_y_pred, zero_division=0)\n",
    "nb_recall = recall_score(y_test, nb_y_pred, zero_division=0)\n",
    "nb_f1 = f1_score(y_test, nb_y_pred, zero_division=0)\n",
    "nb_fpr, nb_tpr, _ = roc_curve(y_test, nb_y_pred_proba)\n",
    "nb_roc_auc = auc(nb_fpr, nb_tpr)\n",
    "nb_cm = confusion_matrix(y_test, nb_y_pred)\n",
    "nb_tn, nb_fp, nb_fn, nb_tp = nb_cm.ravel()\n",
    "print(\"  Accuracy: {:.4f}\".format(nb_accuracy))\n",
    "print(\"  Precision: {:.4f}\".format(nb_precision))\n",
    "print(\"  Recall: {:.4f}\".format(nb_recall))\n",
    "print(\"  F1 Score: {:.4f}\".format(nb_f1))\n",
    "\n",
    "# -------------------------- 4. Train Advanced Model (GradientBoosting) --------------------------\n",
    "print(\"Training GradientBoostingClassifier\")\n",
    "print(\"-\"*40)\n",
    "gb_model = GradientBoostingClassifier(\n",
    "        random_state=42,\n",
    "        n_estimators=100,\n",
    "        max_depth=3,\n",
    "        learning_rate=0.1,\n",
    "        subsample=0.8\n",
    "    )\n",
    "gb_model.fit(X_train, y_train)\n",
    "xgb_y_pred = gb_model.predict(X_test)\n",
    "xgb_y_pred_proba = gb_model.predict_proba(X_test)[:, 1]\n",
    "model_name = \"GradientBoosting\"\n",
    "xgb_accuracy = accuracy_score(y_test, xgb_y_pred)   # Calculate performance metrics for advanced model\n",
    "xgb_precision = precision_score(y_test, xgb_y_pred, zero_division=0)\n",
    "xgb_recall = recall_score(y_test, xgb_y_pred, zero_division=0)\n",
    "xgb_f1 = f1_score(y_test, xgb_y_pred, zero_division=0)\n",
    "xgb_fpr, xgb_tpr, _ = roc_curve(y_test, xgb_y_pred_proba)\n",
    "xgb_roc_auc = auc(xgb_fpr, xgb_tpr)\n",
    "xgb_cm = confusion_matrix(y_test, xgb_y_pred)\n",
    "xgb_tn, xgb_fp, xgb_fn, xgb_tp = xgb_cm.ravel()\n",
    "print(\"{} Performance:\".format(model_name))\n",
    "print(\"  Accuracy: {:.4f}\".format(xgb_accuracy))\n",
    "print(\"  Precision: {:.4f}\".format(xgb_precision))\n",
    "print(\"  Recall: {:.4f}\".format(xgb_recall))\n",
    "print(\"  F1 Score: {:.4f}\".format(xgb_f1))\n",
    "\n",
    "# -------------------------- 5. Model Comparison --------------------------\n",
    "print(\"\\n5. MODEL COMPARISON\")\n",
    "print(\"-\"*40)\n",
    "comparison_data = {\n",
    "    'Model': ['Naive Bayes', model_name],\n",
    "    'Accuracy': [nb_accuracy, xgb_accuracy],\n",
    "    'Precision': [nb_precision, xgb_precision],\n",
    "    'Recall': [nb_recall, xgb_recall],\n",
    "    'F1 Score': [nb_f1, xgb_f1],\n",
    "    'ROC-AUC': [nb_roc_auc, xgb_roc_auc],\n",
    "    'True Positives': [nb_tp, xgb_tp],\n",
    "    'True Negatives': [nb_tn, xgb_tn],\n",
    "    'False Positives': [nb_fp, xgb_fp],\n",
    "    'False Negatives': [nb_fn, xgb_fn]\n",
    "}   # Create comparison table\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "print(\"\\nPERFORMANCE COMPARISON:\")\n",
    "print(\"=\"*90)\n",
    "print(comparison_df.to_string(index=False))\n",
    "print(\"=\"*90)\n",
    "accuracy_improvement = ((xgb_accuracy - nb_accuracy) / nb_accuracy) * 100   # Calculate improvement percentages\n",
    "f1_improvement = ((xgb_f1 - nb_f1) / nb_f1) * 100\n",
    "roc_auc_improvement = ((xgb_roc_auc - nb_roc_auc) / nb_roc_auc) * 100\n",
    "print(\"\\nIMPROVEMENT ANALYSIS:\")\n",
    "print(\"  Accuracy improvement: {:.1f}%\".format(accuracy_improvement))\n",
    "print(\"  F1 Score improvement: {:.1f}%\".format(f1_improvement))\n",
    "print(\"  ROC-AUC improvement: {:.1f}%\".format(roc_auc_improvement))\n",
    "if xgb_accuracy > nb_accuracy:    # Determine which model is better\n",
    "    better_model = model_name\n",
    "    accuracy_diff = xgb_accuracy - nb_accuracy\n",
    "else:\n",
    "    better_model = \"Naive Bayes\"\n",
    "    accuracy_diff = nb_accuracy - xgb_accuracy\n",
    "print(\"\\nCONCLUSION: {} performs better with {:.3f} higher accuracy\".format(better_model, accuracy_diff))\n",
    "\n",
    "# -------------------------- 6. Visualization --------------------------\n",
    "print(\"\\n6. VISUALIZATION\")\n",
    "print(\"-\"*40)\n",
    "fig = plt.figure(figsize=(15, 10))  # Create comprehensive comparison visualization\n",
    "fig.suptitle('{} vs Naive Bayes Model Comparison'.format(model_name), fontsize=16, fontweight='bold')\n",
    "ax1 = plt.subplot(2, 3, 1)   # 6.1 Performance metrics comparison\n",
    "metrics = ['Accuracy', 'Precision', 'Recall', 'F1 Score']\n",
    "nb_scores = [nb_accuracy, nb_precision, nb_recall, nb_f1]\n",
    "xgb_scores = [xgb_accuracy, xgb_precision, xgb_recall, xgb_f1]\n",
    "x = np.arange(len(metrics))\n",
    "width = 0.35\n",
    "ax1.bar(x - width/2, nb_scores, width, label='Naive Bayes', color='lightblue', alpha=0.8)\n",
    "ax1.bar(x + width/2, xgb_scores, width, label=model_name, color='lightgreen', alpha=0.8)\n",
    "ax1.set_xlabel('Metrics')\n",
    "ax1.set_ylabel('Score')\n",
    "ax1.set_title('Performance Metrics Comparison')\n",
    "ax1.set_xticks(x)\n",
    "ax1.set_xticklabels(metrics)\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3, axis='y')\n",
    "ax1.set_ylim([0, 1.1])\n",
    "for i in range(len(metrics)):    # Add value labels\n",
    "    ax1.text(i - width/2, nb_scores[i] + 0.02, '{:.3f}'.format(nb_scores[i]), \n",
    "             ha='center', va='bottom', fontsize=9, fontweight='bold')\n",
    "    ax1.text(i + width/2, xgb_scores[i] + 0.02, '{:.3f}'.format(xgb_scores[i]), \n",
    "             ha='center', va='bottom', fontsize=9, fontweight='bold')\n",
    "ax2 = plt.subplot(2, 3, 2)   # 6.2 ROC Curves comparison\n",
    "ax2.plot(nb_fpr, nb_tpr, color='blue', lw=2, label='Naive Bayes (AUC = {:.3f})'.format(nb_roc_auc))\n",
    "ax2.plot(xgb_fpr, xgb_tpr, color='green', lw=2, label='{} (AUC = {:.3f})'.format(model_name, xgb_roc_auc))\n",
    "ax2.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random Classifier')\n",
    "ax2.set_xlim([0.0, 1.0])\n",
    "ax2.set_ylim([0.0, 1.05])\n",
    "ax2.set_xlabel('False Positive Rate')\n",
    "ax2.set_ylabel('True Positive Rate')\n",
    "ax2.set_title('ROC Curve Comparison')\n",
    "ax2.legend(loc=\"lower right\")\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax3 = plt.subplot(2, 3, 3)   # 6.3 Confusion matrices\n",
    "sns.heatmap(nb_cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['Predicted Low', 'Predicted High'],\n",
    "            yticklabels=['Actual Low', 'Actual High'], ax=ax3)\n",
    "ax3.set_title('Naive Bayes - Confusion Matrix')\n",
    "ax3.set_ylabel('True Label')\n",
    "ax3.set_xlabel('Predicted Label')\n",
    "ax4 = plt.subplot(2, 3, 4)\n",
    "sns.heatmap(xgb_cm, annot=True, fmt='d', cmap='Greens', \n",
    "            xticklabels=['Predicted Low', 'Predicted High'],\n",
    "            yticklabels=['Actual Low', 'Actual High'], ax=ax4)\n",
    "ax4.set_title('{} - Confusion Matrix'.format(model_name))\n",
    "ax4.set_ylabel('True Label')\n",
    "ax4.set_xlabel('Predicted Label')\n",
    "ax5 = plt.subplot(2, 3, 5)   # 6.4 Error type comparison\n",
    "error_types = ['False Positives', 'False Negatives']\n",
    "nb_errors = [nb_fp, nb_fn]\n",
    "xgb_errors = [xgb_fp, xgb_fn]\n",
    "x = np.arange(len(error_types))\n",
    "ax5.bar(x - width/2, nb_errors, width, label='Naive Bayes', color='lightblue', alpha=0.8)\n",
    "ax5.bar(x + width/2, xgb_errors, width, label=model_name, color='lightgreen', alpha=0.8)\n",
    "ax5.set_xlabel('Error Type')\n",
    "ax5.set_ylabel('Count')\n",
    "ax5.set_title('Error Type Comparison')\n",
    "ax5.set_xticks(x)\n",
    "ax5.set_xticklabels(error_types)\n",
    "ax5.legend()\n",
    "ax5.grid(True, alpha=0.3, axis='y')\n",
    "ax6 = plt.subplot(2, 3, 6)   # 6.5 Prediction Probability Distribution\n",
    "nb_risk_probs = nb_y_pred_proba[y_test == 1]    # High-risk group predictions\n",
    "xgb_risk_probs = xgb_y_pred_proba[y_test == 1]\n",
    "ax6.hist(nb_risk_probs, bins=20, alpha=0.7, label='Naive Bayes', color='lightblue', density=True)\n",
    "ax6.hist(xgb_risk_probs, bins=20, alpha=0.7, label=model_name, color='lightgreen', density=True)\n",
    "ax6.set_xlabel('Predicted High Risk Probability')\n",
    "ax6.set_ylabel('Density')\n",
    "ax6.set_title('Prediction Distribution (High Risk Group)')\n",
    "ax6.legend()\n",
    "ax6.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# -------------------------- 7. Detailed Analysis --------------------------\n",
    "print(\"\\n7. DETAILED ANALYSIS\")\n",
    "print(\"-\"*40)\n",
    "print(\"\\nNaive Bayes Classification Report:\")\n",
    "print(\"-\"*40)\n",
    "print(classification_report(y_test, nb_y_pred, target_names=['Low Risk', 'High Risk']))\n",
    "print(\"\\n{} Classification Report:\".format(model_name))\n",
    "print(\"-\"*40)\n",
    "print(classification_report(y_test, xgb_y_pred, target_names=['Low Risk', 'High Risk']))\n",
    "print(\"\\nERROR ANALYSIS:\")\n",
    "print(\"-\"*40)\n",
    "disagreement_mask = (nb_y_pred != xgb_y_pred)   # Find samples where models disagree\n",
    "agreement_mask = (nb_y_pred == xgb_y_pred)\n",
    "print(\"Samples where models agree: {} ({:.1f}%)\".format(agreement_mask.sum(), agreement_mask.sum()/len(y_test)*100))\n",
    "print(\"Samples where models disagree: {} ({:.1f}%)\".format(disagreement_mask.sum(), disagreement_mask.sum()/len(y_test)*100))\n",
    "if disagreement_mask.sum() > 0:\n",
    "    nb_wrong_xgb_right = (nb_y_pred != y_test.values) & (xgb_y_pred == y_test.values)\n",
    "    xgb_wrong_nb_right = (xgb_y_pred != y_test.values) & (nb_y_pred == y_test.values)\n",
    "    print(\"  Cases where Naive Bayes is wrong but {} is right: {}\".format(model_name, nb_wrong_xgb_right.sum()))\n",
    "    print(\"  Cases where {} is wrong but Naive Bayes is right: {}\".format(model_name, xgb_wrong_nb_right.sum()))\n",
    "\n",
    "# -------------------------- 8. Recommendations --------------------------\n",
    "print(\"\\n8. RECOMMENDATIONS\")\n",
    "print(\"-\"*40)\n",
    "print(\"\\nWHEN TO USE NAIVE BAYES:\")\n",
    "print(\"-\"*40)\n",
    "print(\"1. When you need a simple baseline model\")\n",
    "print(\"2. When training time is critical\")\n",
    "print(\"3. When interpretability is important\")\n",
    "print(\"4. When you have limited computational resources\")\n",
    "print(\"5. When features are relatively independent\")\n",
    "print(\"\\nWHEN TO USE {}:\".format(model_name.upper()))\n",
    "print(\"-\"*40)\n",
    "print(\"1. When accuracy is the primary concern\")\n",
    "print(\"2. When dealing with complex, non-linear relationships\")\n",
    "print(\"3. When you have sufficient computational resources\")\n",
    "print(\"4. When feature interactions are important\")\n",
    "print(\"5. For production systems where performance matters\")\n",
    "print(\"\\nFOR THIS MENTAL HEALTH PREDICTION TASK:\")\n",
    "print(\"-\"*40)\n",
    "print(\"1. {} provides accuracy: {:.3f} vs Naive Bayes: {:.3f}\".format(model_name, xgb_accuracy, nb_accuracy))\n",
    "print(\"2. {} handles feature interactions better for complex mental health patterns\".format(model_name))\n",
    "print(\"3. Naive Bayes is faster and easier to interpret\")\n",
    "print(\"4. Recommendation: Use {} for final deployment, Naive Bayes for quick analysis\".format(model_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tianqi Yin (conclusions below for her part):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Findings\n",
    "1. **Happiness Predictability**: Achieved 74% accuracy in distinguishing low, medium, and high happiness levels using limited features\n",
    "2. **Key Driving Factors**: Sleep quality and stress level are the strongest predictors for differentiating happiness levels\n",
    "3. **Method Efficiency**: Linear Discriminant Analysis (LDA) proved sufficiently effective, eliminating the need for complex nonlinear models\n",
    "\n",
    "## Theoretical Contributions\n",
    "- Validated the **measurability** and **distinguishability** of happiness\n",
    "- Provided **feature importance ranking**: Sleep quality > Stress level > Other factors\n",
    "- Demonstrated **dimensionality reduction effectiveness**: 13 features can be compressed into 1-2 core dimensions\n",
    "\n",
    "## Practical Implications\n",
    "1. **Mental Health Assessment**: Simple models can be used for preliminary screening\n",
    "2. **Intervention Design**: Priority should be given to sleep and stress management\n",
    "3. **Data Collection**: Questionnaires can be optimized to focus on key questions\n",
    "\n",
    "## Limitations\n",
    "1. **Medium Happiness Difficult to Distinguish**: May require finer-grained measurement\n",
    "2. **Linear Assumption**: May miss complex nonlinear relationships\n",
    "3. **Sample Representativeness**: Requires larger sample validation\n",
    "\n",
    "## Future Directions\n",
    "1. Incorporate more **contextual features** (work environment, social support, etc.)\n",
    "2. Explore **time-series analysis** (happiness change trends)\n",
    "3. Develop **personalized prediction models**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our findings suggest that everyday social-media habits are not only a matter of “time spent online,” but are deeply connected to how students use digital spaces to present and explore who they are. When moderate screen time and regular exercise co-occur, our PCA, Lasso, and logistic models show higher well-being scores and a higher probability of falling into the “good status” group, whereas very long daily screen time is associated with a sharp drop in predicted well-being. This pattern fits the idea that online communication can activate and support the “true self,” but only under conditions where use is manageable and does not crowd out offline coping resources (Bargh et al., 2002). At the same time, our data speak to work on impression management and self-presentation: students may invest heavy time in certain platforms to maintain curated identities and audiences, which can bring social rewards but also increase pressure and comparison (Bullingham & Vasconcelos, 2013; Chu & Choi, 2010). Put together, the projects show that the same platforms that help people express authentic aspects of self and build social capital can also become risky when use exceeds a certain threshold, pointing to the importance of interventions that focus not only on “less screen time,” but on healthier patterns of self-presentation, community choice, and offline balance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6 Individual Contribution\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "1. Yifei Pan: \n",
    "Project coordination & pipeline design. Primary responsibility for Logistic Regression model development, interpretation (coefficients/odds ratios), and risk pattern analysis based on its predictions. Leads writing for Introduction and key Results sections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Ruanyiyang Sun: \n",
    "Data exploration & descriptive analysis. Primary responsibility for Naive Bayes model development, analysis of its error patterns and limitations. Leads writing for EDA and model comparison sections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Tianqi Yin: \n",
    "Data cleaning & preprocessing. Primary responsibility for LDA model development, visualization of class separation, and fairness analysis. Leads writing for Methods (feature engineering, LDA) and Limitations sections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "Bargh, J. A., McKenna, K. Y. A., & Fitzsimons, G. M. (2002). Can you see the real me? Activation and expression of the “true self” on the Internet. Journal of Social Issues, *58*(1), 33–48.\n",
    "\n",
    "Bullingham, L., & Vasconcelos, A. C. (2013). “The presentation of self in the online world”: Goffman and the study of online identities. Journal of Information Science, *39*(1), 101–112.\n",
    "\n",
    "Chu, S. C., & Choi, S. M. (2010). Social capital and self-presentation of young generations in online community sites. International Journal of Internet Marketing and Advertising, *6*(2), 120–145.\n",
    "\n",
    "Rajak, P. (2024). Mental Health & Social Media Balance Dataset [Data set]. Kaggle. https://www.kaggle.com/datasets/prince7489/mental-health-and-social-media-balance-dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix\n",
    "\n",
    "Code and extra figures can be put here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note on code placement.**  \n",
    "Some code snippets that would normally be placed in the appendix are integrated into the *Results* and *Conclusion* sections. We chose this structure because the interpretation of our findings depends directly on the modeling steps and parameter choices, and presenting the relevant code alongside the results makes the analysis more transparent and easier to follow for the reader. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
